{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "<table align=\"left\" width=100%>\n",
    "    <td>\n",
    "        <div style=\"text-align: center;\">\n",
    "          <img src=\"./images/bar.png\" alt=\"entidades financiadoras\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <p style=\"text-align: center; font-size:24px;\"><b>Introduction to Data Science</b></p>\n",
    "        <p style=\"text-align: center; font-size:18px;\"><b>Master in Electrical and Computer Engineering</b></p>\n",
    "        <p style=\"text-align: center; font-size:14px;\"><b>Pedro Cardoso (pcardoso@ualg.pt)</b></p>\n",
    "    </td>\n",
    "</table>\n",
    "\n",
    "_____\n",
    "\n",
    "__Short Lesson Title:__ Advanced Data Transformation and Feature Engineering\n",
    "\n",
    "*__Summary:__ This lesson delves into advanced data manipulation techniques within the realm of Exploratory Data Analysis (EDA), building upon previous lessons. It focuses on data type management, feature encoding, feature scaling, feature transformation, and feature splitting. Students will learn how to effectively manage different data types in pandas, including converting between numerical, categorical, and boolean types, and understanding the performance implications of each. The lesson covers one-hot encoding and label encoding for categorical variables, as well as standard, min-max, and robust scaling for numerical features. It also explores feature transformation techniques like log and square root transformations to address skewness. Finally, the lesson introduces feature splitting, demonstrating how to extract meaningful information from existing features. Through practical examples, students will gain skills in preparing data for advanced analysis and modeling.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with Pandas (part 3)\n",
    "\n",
    "Let us continue with the exploratory data analysis with Pandas. We will continue with the titanic dataset. So, load the titanic dataset and remember its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:43:05.893267Z",
     "start_time": "2025-04-22T12:43:05.216080Z"
    }
   },
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "\n",
    "pandas_bokeh.output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:43:05.995110Z",
     "start_time": "2025-04-22T12:43:05.942094Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic = pd.read_excel('./data/titanic/Titanic.xls')\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types in Pandas\n",
    "\n",
    "Pandas has a `dtypes` attribute that returns the data types of the columns in the data frame. Possible data types include\n",
    "- `int64`, \n",
    "- `float64`, \n",
    "- `object`, \n",
    "- `bool`, \n",
    "- `datetime64`, \n",
    "- `timedelta[ns]`, and \n",
    "- `category`. \n",
    "\n",
    "By default, the data types are inferred from the data. If the data type is numeric, numpy data types are used. If the data type is non-numeric, the data type is inferred as `object`. The `object` data type is used for string values and for other mixed data types. (see https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes for more details)\n",
    "\n",
    "For example, let us check the data types of the titanic dataset using the `dtypes` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:43:06.780110Z",
     "start_time": "2025-04-22T12:43:06.768260Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be also visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:43:07.659886Z",
     "start_time": "2025-04-22T12:43:07.558127Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the data types of a specific column using the `dtype` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:43:09.170152Z",
     "start_time": "2025-04-22T12:43:09.060808Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic['survived'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get all columns of a specific data type using the `select_dtypes` method.\n",
    "\n",
    "Such as all columns of type `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-22T12:43:09.572180Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or all columns of type `float64`.\n",
    "\n",
    "As a note, `body` is considered as a float because it contains missing values, i.e., `NaN` (because `NaN` is a float, this forces an array of integers with any missing values to become floating point). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:27.377464Z",
     "start_time": "2025-04-22T12:31:27.349849Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or all columns of type `object`.\n",
    "Object data type is used for string values and for other mixed data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:27.516487Z",
     "start_time": "2025-04-22T12:31:27.501632Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or, all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:27.624408Z",
     "start_time": "2025-04-22T12:31:27.619099Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion of the data types can be done using the `astype` method. For example, let us convert the `survived` feature (containing 0 and 1) to `bool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:27.701837Z",
     "start_time": "2025-04-22T12:31:27.697684Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# convert the survived feature to bool\n",
    "df['survived'] = df['survived'].astype(bool)\n",
    "\n",
    "# check the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, we can see that the `survived` feature is now of type `bool`, i.e., `True` and `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:27.824442Z",
     "start_time": "2025-04-22T12:31:27.820643Z"
    }
   },
   "outputs": [],
   "source": [
    "df['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other transformations are natural such as converting the `embarked` and `sex` feature to `category`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:27.960678Z",
     "start_time": "2025-04-22T12:31:27.956181Z"
    }
   },
   "outputs": [],
   "source": [
    "df['embarked_category'] = df['embarked'].astype('category')\n",
    "df['sex_category'] = df['sex'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not change the values of the feature, but it changes the data type, the `pclass` feature is now of type `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:28.047507Z",
     "start_time": "2025-04-22T12:31:28.041940Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['embarked_category', 'sex_category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pcalss` feature can also be considered as a categorical feature. Let us convert it to `category` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:31:28.169626Z",
     "start_time": "2025-04-22T12:31:28.165727Z"
    }
   },
   "outputs": [],
   "source": [
    "df['pclass_category'] = df['pclass'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the `category` data type vs. `object` data type (optional)\n",
    "\n",
    "This is useful for performance reasons, as in some cases, the `category` data type is more efficient than `object` data type. For example, let us compare the performance of the `category` and `object` data types when counting the number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:32:12.648755Z",
     "start_time": "2025-04-22T12:31:28.228561Z"
    }
   },
   "outputs": [],
   "source": [
    "def timeit_cat_vs_other(df, feature):\n",
    "    print()\n",
    "    feature_category = f'{feature}_category'\n",
    "    print(f'Time to count the number of unique values in the {feature} feature (dtype = {df[feature].dtype}) vs. {df[feature_category].dtype})')\n",
    "    %timeit df[feature].nunique()\n",
    "    %timeit df[feature_category].nunique()\n",
    "\n",
    "timeit_cat_vs_other(df, 'sex')\n",
    "timeit_cat_vs_other(df, 'embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in terms of memory usage, the `category` data type is more efficient than `object` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:32:12.661853Z",
     "start_time": "2025-04-22T12:32:12.653818Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['embarked', 'embarked_category', 'sex', 'sex_category', 'pclass', 'pclass_category']].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations of categorical columns can also have significant impact on the performance of some methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:32:37.126350Z",
     "start_time": "2025-04-22T12:32:12.698653Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Time to convert the sex feature to upper case')\n",
    "%timeit df['sex'].str.upper()\n",
    "%timeit df['sex_category'].str.upper()\n",
    "\n",
    "print()\n",
    "print('Time to convert the embarked feature to upper case')\n",
    "%timeit df['embarked'].str.upper()\n",
    "%timeit df['embarked_category'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other operations the `category` data type is **not** more efficient than `object` data type. For example, let us compare the performance of the `category`, `object` and `int64` a data types when counting the number of occurrences of each value. \n",
    "\n",
    "This is because the `category` data type is stored as an array of integers, and the `object` data type is stored as an array of pointers to the strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:32:57.783667Z",
     "start_time": "2025-04-22T12:32:37.142752Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Time to count the number of occurrences of each value in the embarked feature')\n",
    "%timeit df['embarked'].value_counts()\n",
    "%timeit df['embarked_category'].value_counts()\n",
    "\n",
    "print()\n",
    "print('Time to count the number of occurrences of each value in the pclass (!int64!) feature')\n",
    "%timeit df['pclass'].value_counts()\n",
    "%timeit df['pclass_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or when doing a group by operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:18.371121Z",
     "start_time": "2025-04-22T12:32:57.786927Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Time to group by the embarked feature')\n",
    "%timeit df.groupby('embarked').size()\n",
    "%timeit df.groupby('embarked_category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:34.169136Z",
     "start_time": "2025-04-22T12:33:18.478956Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Time to group by pclass feature (!int64!)')\n",
    "%timeit df.groupby('pclass').size()\n",
    "%timeit df.groupby('pclass_category').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the conversion of the data types can have significant impact on the performance. In general, the `category` data type is more efficient than `object` data type, but not always. This must be evaluated on a case by case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding\n",
    "Feature encoding is the process of transforming the features to have a more machine learning friendly format. For example, categorical features are transformed to have integer values. This is useful for some machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding is the process of transforming the features to have a more machine learning friendly format. For example, categorical features are split into multiple binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:34.187028Z",
     "start_time": "2025-04-22T12:33:34.176011Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_titanic.copy()\n",
    "\n",
    "encoded_data = pd.get_dummies(\n",
    "    df[['embarked', 'sex']],\n",
    "    dtype=int # default is bool\n",
    ")\n",
    "\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:34.224761Z",
     "start_time": "2025-04-22T12:33:34.216119Z"
    }
   },
   "outputs": [],
   "source": [
    "# add the encoded data to the original data frame and drop the original features\n",
    "df = pd.concat([df, encoded_data], axis=1).drop(['embarked', 'sex'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "Label encoding is the process of transforming the features to, usually, have a more machine learning friendly format. For example, categorical features are transformed to have integer values.\n",
    "\n",
    "Let us encode the `embarked` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:36.161248Z",
     "start_time": "2025-04-22T12:33:34.404236Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# encode the embarked feature\n",
    "le = LabelEncoder()\n",
    "df['embarked'] = le.fit_transform(df['embarked'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LabelEncoder` class has a `classes_` attribute that contains the list of classes that were encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:36.228721Z",
     "start_time": "2025-04-22T12:33:36.225418Z"
    }
   },
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to `LabelEncoder`, for categorial features we can use the `cat.codes` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:36.301422Z",
     "start_time": "2025-04-22T12:33:36.297551Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_titanic.copy()\n",
    "df['embarked'] = df['embarked'].astype('category')\n",
    "df['embarked'] = df['embarked'].cat.codes\n",
    "df['embarked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal encoding\n",
    "\n",
    "Ordinal encoding is similar to label encoding, but the classes are ordered. For example, the classes `low`, `medium`, and `high` can be encoded as `0`, `1`, and `2`, respectively.\n",
    "\n",
    "In the Titanic dataset, we can consider the `pclass` feature as an ordinal feature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:36.592526Z",
     "start_time": "2025-04-22T12:33:36.587017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_titanic.pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "In practical scenarios, features often have distinct ranges, magnitudes, and units. For instance, age may vary between 0 and 120, while salary can fluctuate between zero and thousands or even millions. This raises the question of how data analysts or scientists can compare such features, given that they are on different scales. It is worth noting that high-magnitude features tend to have a more significant impact on machine learning models than lower magnitude ones. Fortunately, feature scaling or normalization can help address these issues.\n",
    "\n",
    "So, feature scaling refers to the process of bringing all features to the same magnitude level. It is not mandatory for all algorithms, but some algorithms necessitate scaled data, such as those that depend on Euclidean distance measures, like K-nearest neighbor and K-means clustering algorithms.\n",
    "\n",
    "Scalllin can also be used to annoniymize the data. For example, the age of a person can be scaled to the range [0, 1] by dividing by 100. This way, the age of a person is not directly available in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard scaling\n",
    "Standard scaling is the process of transforming the features to have a more normal distribution. The standard scaling is performed by subtracting the mean and dividing by the standard deviation., i.e., $$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:36.915661Z",
     "start_time": "2025-04-22T12:33:36.822069Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# standard scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[features])\n",
    "\n",
    "\n",
    "df[features] = scaler.transform(df[features])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us see the distribution of the original data and the scaled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:38.383442Z",
     "start_time": "2025-04-22T12:33:37.834477Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='standard scaled data', ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distribution of the original data and the scaled data is different. The scaled data has a mean of zero and a standard deviation of one. The scaled data is centered around zero, and the values are almost all within the range of -3 and 3. The scaled data is more normally distributed than the original data.\n",
    "\n",
    "However, plotting the data, we can see that the shape of the data is the same, only the scale is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:38.791121Z",
     "start_time": "2025-04-22T12:33:38.558348Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = df_titanic['age'].sort_values().reset_index().drop('index', axis=1)\\\n",
    "    .plot(style='o', title='original data')\n",
    "    \n",
    "df['age'].sort_values().reset_index().drop('index', axis=1)\\\n",
    "    .plot(style=\".\",ax=ax, secondary_y=True)\n",
    "    \n",
    "ax.set_ylabel('original data')\n",
    "ax.right_ax.set_ylabel('standard scaled data')\n",
    "ax.set_xlabel('index')\n",
    "ax.set_title('original data vs. standard scaled data')\n",
    "ax.right_ax.legend(['original data'], loc='upper left')\n",
    "ax.legend(['original data', 'standard scaled data'], loc='upper right')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max scaling\n",
    "\n",
    "Min-max scaling is the process of transforming the features to have a more uniform distribution. The min-max scaling to the [0, 1] interval is performed by subtracting the minimum and dividing by the range., i.e., $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}.$$\n",
    "\n",
    "To transform the data to the $[a, b]$ interval, we can use the following formula: $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}  (b - a) + a.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:39.494764Z",
     "start_time": "2025-04-22T12:33:39.465978Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us see the distribution of the original data and the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:40.707749Z",
     "start_time": "2025-04-22T12:33:40.368149Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='min-max scaled data', ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distribution of the original data and the scaled data is different. The scaled data is in the [0, 1] interval. The scaled data is more uniformly distributed than the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust scaling\n",
    "Robust scaling features in a way that accounts for outliers. The method achieves this by first removing the median and then scaling the data based on the quantile range. The default quantile range used is the Interquartile Range (IQR), although it can be customized if needed.\n",
    "\n",
    "During the scaling process, each feature is centered and scaled independently by computing relevant statistics from the training set. Outliers can often skew the sample mean and variance in undesirable ways.\n",
    "\n",
    "So, robust scaling is computed as follows:\n",
    " $$x_{scaled} = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)},$$\n",
    "where $\\text{median}(x)$ is the median of the feature $x$, and $\\text{IQR}(x)$ is the interquartile range of the feature $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:41.294672Z",
     "start_time": "2025-04-22T12:33:41.166067Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# robust scaling\n",
    "scaler = RobustScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us see the distribution of the original data and the scaled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:42.260501Z",
     "start_time": "2025-04-22T12:33:41.813243Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='robust scaled data', ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distribution of the original data and the scaled data is different. The scaled data is centered around zero and is more uniformly distributed than the original data. The scaled data is more robust to outliers than the original data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "Feature transformation is the process of transforming the features allows, for exameple, reducing the skewness of the features, the effect of outliers, etc. Examples feature transformations are log transformation, square root transformation, square, etc.\n",
    "\n",
    "\n",
    "This kind of transformation is useful for:\n",
    "- some **machine learning algorithms and for some statistical tests**. For example, the t-test assumes that the data is normally distributed. If the data is not normally distributed, then the t-test may not be valid. In this case, we can apply a transformation to the data to make it more normally distributed.\n",
    "- **to reduce the effect of outliers**. For example, if we have a feature that has a few very large values, then the mean and standard deviation of the feature will be affected by these outliers. In this case, we can apply a transformation to the data to reduce the effect of these outliers.\n",
    "- **to reduce the skewness of the data**. For example, if we have a feature that is right-skewed, then the mean will be larger than the median. In this case, we can apply a transformation to the data to make it more normally distributed. If the feature is right-skewed or positively skewed or grouped at lower values, then we can apply the square root, cube root, and logarithmic transformations. If the feature is left-skewed or negative skewed or grouped at higher values, then common practice is to reflect the data first (e.g., subtract each value from a constant larger than the maximum value), making it right-skewed, and then apply the same transformations (log, square root, etc.) as you would for right-skewed data.\n",
    "- **to reduce the effect of heteroscedasticity**. For example, In many datasets, the variance increases with the mean (heteroscedasticity). Log transformation helps stabilize variance across the range of data, making statistical modeling more robust and reliable. This is critical for meeting the assumptions of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Log transformation\n",
    "\n",
    "The log transformation is performed by taking the logarithm of the feature., i.e., $$x_{log} = \\log(x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:42.571701Z",
     "start_time": "2025-04-22T12:33:42.562798Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# log transformation\n",
    "df[features] = df[features].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:43.988840Z",
     "start_time": "2025-04-22T12:33:42.622848Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='log scaled data', ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square root transformation\n",
    "Square root transformation is performed by taking the square root of the feature., i.e., $$x_{sqrt} = \\sqrt{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:44.876406Z",
     "start_time": "2025-04-22T12:33:44.852641Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# square transformation\n",
    "df[features] = df[features].apply(lambda x: np.sqrt(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:45.555864Z",
     "start_time": "2025-04-22T12:33:45.115835Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='square scaled data', ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization transformation\n",
    "Discretization transformation is performed by transforming numerical features to categorical features. This is useful for some machine learning algorithms. For example, the following code transforms the age feature to a categorical feature.\n",
    "\n",
    "To do this, we use the `pd.cut` function. The `pd.cut` function takes as input the feature to be transformed, the bins, and the labels. The bins are the intervals in which the feature will be transformed. The labels are the names of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:45.848903Z",
     "start_time": "2025-04-22T12:33:45.823739Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# discretization transformation\n",
    "df['age category'] = pd.cut(df['age'], bins=[0, 18, 30, 65, 100], labels=['child', 'young', 'adult', 'senior'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:46.079212Z",
     "start_time": "2025-04-22T12:33:45.904864Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['age', 'age category']].groupby('age category', observed=True).count().plot(kind='bar')\n",
    "plt.title('Age categories')\n",
    "plt.xlabel('Age category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature splitting\n",
    "Feature splitting is the process of splitting a feature into multiple features. For example, some times is possible to split the `name` feature into two features: `first name` and `last name`. Or the spliting of a `date` feature into three features: `year`, `month`, and `day`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dates example\n",
    "\n",
    "Consider the dataset with energy consumption of a house. The dataset contains the date and the energy consumption of the house. The date feature can be split into multiple features, such as year, month, day, hour, minute, and second. This can be done using the `dt` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:46.709243Z",
     "start_time": "2025-04-22T12:33:46.536453Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_consumption = pd.read_csv('./data/house_consumption_TS/house_consumption.csv')\n",
    "df_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:47.143100Z",
     "start_time": "2025-04-22T12:33:47.114224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_consumption.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:47.507924Z",
     "start_time": "2025-04-22T12:33:47.490495Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_consumption['date'] = pd.to_datetime(df_consumption['date'])\n",
    "df_consumption.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:47.534845Z",
     "start_time": "2025-04-22T12:33:47.519374Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_consumption['year'] = df_consumption['date'].dt.year\n",
    "df_consumption['month'] = df_consumption['date'].dt.month\n",
    "df_consumption['day'] = df_consumption['date'].dt.day\n",
    "df_consumption['hour'] = df_consumption['date'].dt.hour\n",
    "df_consumption['minute'] = df_consumption['date'].dt.minute\n",
    "df_consumption['second'] = df_consumption['date'].dt.second\n",
    "df_consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Titanic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Titanic dataset, we can split the cabin feature into two features: `cabin number` and `cabin letter`. The latter corresponds to the deck of the Titanic. Furhter, some passenger have more than one cabin. In this case, we can split the cabin feature into multiple features, one for each cabin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T12:33:47.591572Z",
     "start_time": "2025-04-22T12:33:47.584039Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `extract` function can be used to extract the deck from the cabin feature. The `extract` function takes as input a regular expression that defines the pattern to be extracted.\n",
    "\n",
    "See notebook [./04_d_exploratory_data_analysis.ipynb](./04_d_exploratory_data_analysis.ipynb)\n",
    "\n",
    "For example, the following code extracts the deck from the cabin feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['cabin']\n",
    "\n",
    "# feature splitting\n",
    "# ([0-9]+): one or more digits\n",
    "df['cabin number'] = df['cabin'].str.extract('([0-9]+)')\n",
    "\n",
    "# ([A-Z]): one upper case letter\n",
    "df['deck'] = df['cabin'].str.extract('([A-Z])')\n",
    "\n",
    "# count the number of cabins, because the \"cabin\" column may contain multiple cabins\n",
    "df['number of cabins'] = df['cabin'].str.split().str.len()\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it can be interesting to see the distribution of the passenger class in different decks. This can be done using a pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_class_by_deck = df[['pclass', 'deck']].pivot_table(index='deck', columns='pclass', aggfunc='size', fill_value=0)\n",
    "\n",
    "passenger_class_by_deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_class_by_deck.plot(kind=\"bar\")\n",
    "plt.title('Decks')\n",
    "plt.xlabel('Deck')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can check is the correlation between the number of cabins and the passenger class or the fare. We can see that the number of cabins is highly correlated fare but not with the passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['number of cabins', 'pclass', 'fare']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the distribution of the mean fare in different passenger classes and number of cabins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='pclass', columns='number of cabins', values='fare', aggfunc='mean', fill_value=0)#.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let me give you some examples related to the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "name = 'Braund, Mr. Owen Harris & Bradley, Mrs. Anna Michaela'\n",
    "\n",
    "def print_regex_result(explanation, name, regex):\n",
    "    print(re.findall(regex, name), \":\", explanation)\n",
    "\n",
    "print_regex_result('title, extracted considering the dot ', name, r'([A-Za-z]+)\\.')\n",
    "print_regex_result('title, extracted considering the dot ', name, r'(\\w+)\\.')\n",
    "print_regex_result('title, extracted considering the comma', name, r',\\s([A-Za-z]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_regex_result('last name, extracted considering the comma', name, r'([A-Za-z]+),')\n",
    "print_regex_result( 'first name, extracted considering the dot', name, r'\\.\\s([A-Za-zç]+)')\n",
    "print_regex_result('first name and posterior name, extracted considering the space between words', name, r'([A-Za-zç]+)\\s([A-Za-z]+)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_regex_result('list of words', name, r'([A-Za-z]+)')\n",
    "print_regex_result('list of words', name, r'(\\w+)')\n",
    "print_regex_result('list of names', name, r'([A-Za-z]+),\\s\\w+.\\s([A-Za-z]+)\\s([A-Za-z]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Feature engineering includes:\n",
    "- the creation of new features from the existing features\n",
    "- the selection of features\n",
    "- the extraction of features \n",
    "- the reduction of features, and \n",
    "- the aggregation of features. \n",
    "                \n",
    "We already saw how to create new features from the existing features, e.g., by extracting the deck from the cabin feature.\n",
    "\n",
    "As another example, we can create a new columns with the title and last name of the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['name']\n",
    "\n",
    "# feature engineering\n",
    "df['title'] = df['name'].str.extract(r'([A-Za-z]+)\\.')\n",
    "df['last name'] = df['name'].str.extract(r'([A-Za-z]+),')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "### Correlation as a feature selection technique\n",
    "In feature selection we select the features that are useful for the analysis. We can select the features using the correlation with the target variable. For example, we can select the features that have a correlation with the target variable greater than 0.1 to predict the survival of the passenger. \n",
    "\n",
    "Using features with high correlation with the target variable can help to improve the performance of the model. \n",
    "\n",
    "On the other hand, using features correlated to each other(excluding the target) might not help to improve the performance of the model. For example, if we have two features that are highly correlated, then we can select one of them and discard the other. This is because the two features contain the same information. In this case, we can use the `drop` method to drop one of the features.\n",
    "\n",
    "The use of correlation as a feature selection technique is not always the best approach. For example, if we have two features that are highly correlated, then we can select one of them and discard the other. This is because the two features contain the same information. In this case, we can use the `drop` method to drop one of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the numerical features\n",
    "df = df_titanic.select_dtypes(include=np.number)\n",
    "\n",
    "# compute the correlation with the target variable\n",
    "df.corr()['survived'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using the variance\n",
    "We can also select the features using the variance. For example, we can select the features that have a variance greater than 1 to predict the survival of the passenger.\n",
    "\n",
    "Using features with high variance can help to improve the performance of the model because they contain more information.\n",
    "\n",
    "Note that, due to the magnitude of the values, the variance of the features is not always a good measure of the information contained in the feature. In that case, we can use other methods such as feature importance which is a model based ranking associated to models such as Random Forest, XGBoost, etc. Also, the use of Principal Component Analysis (PCA) can be used to select the features. PCA is a dimensionality reduction technique that can be used to reduce the number of features in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the numerical features\n",
    "df = df_titanic.select_dtypes(include=np.number)\n",
    "\n",
    "# compute the variance\n",
    "df.var().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the variance is magnitude dependent. This means that the variance of a feature is affected by the scale of the feature. For example, if we have a feature that has a range of [0, 1], then the variance will be small. If we have a feature that has a range of [0, 100], then the variance will be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features with variance greater than 0.1\n",
    "df = df.loc[:, df.var() > 1]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Campesato, O. (2018). Regular expressions: Pocket primer. Mercury Learning and Information.\n",
    "- https://www.kaggle.com/learn/pandas\n",
    "- Navlani, A.,  Fandango, A.,  Idris, I. (2021). Python Data Analysis: Perform data collection, data processing, wrangling, visualization, and model building using Python. Packt. 3rd Edition\n",
    "- Brandt. S. (2014). Data Analysis: Statistical and Computational Methods for Scientists and Engineers. Springer. 4th Edition\n",
    "- https://eugenelohh.medium.com/data-analysis-on-the-titanic-dataset-using-python-7593633135f2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_IDC_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
