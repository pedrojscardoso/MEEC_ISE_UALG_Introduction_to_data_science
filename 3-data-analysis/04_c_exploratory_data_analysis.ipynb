{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_____\n",
    "\n",
    "<table align=\"left\" width=100%>\n",
    "    <td>\n",
    "        <div style=\"text-align: center;\">\n",
    "          <img src=\"./images/bar.png\" alt=\"entidades financiadoras\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <p style=\"text-align: center; font-size:24px;\"><b>Introduction to Data Science</b></p>\n",
    "        <p style=\"text-align: center; font-size:18px;\"><b>Master in Electrical and Computer Engineering</b></p>\n",
    "        <p style=\"text-align: center; font-size:14px;\"><b>Pedro Cardoso (pcardoso@ualg.pt)</b></p>\n",
    "    </td>\n",
    "</table>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with Pandas (part 3)\n",
    "\n",
    "Let us continue with the exploratory data analysis with Pandas. We will continue with the titanic dataset. So, load the titanic dataset and remember its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "\n",
    "pandas_bokeh.output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = pd.read_excel('./data/titanic/Titanic.xls')\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types in Pandas\n",
    "\n",
    "Pandas has a `dtypes` attribute that returns the data types of the columns in the data frame. Possible data types include\n",
    "- `int64`, \n",
    "- `float64`, \n",
    "- `object`, \n",
    "- `bool`, \n",
    "- `datetime64`, \n",
    "- `timedelta[ns]`, and \n",
    "- `category`. \n",
    "\n",
    "By default, the data types are inferred from the data. If the data type is numeric, numpy data types are used. If the data type is non-numeric, the data type is inferred as `object`. The `object` data type is used for string values and for other mixed data types. (see https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes for more details)\n",
    "\n",
    "For example, let us check the data types of the titanic dataset using the `dtypes` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be also visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the data types of a specific column using the `dtype` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic['survived'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get all columns of a specific data type using the `select_dtypes` method.\n",
    "\n",
    "Such as all columns of type `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or all columns of type `float64`.\n",
    "\n",
    "As a note, `body` is considered as a float because it contains missing values, i.e., `NaN` (because `NaN` is a float, this forces an array of integers with any missing values to become floating point). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or all columns of type `object`.\n",
    "Object data type is used for string values and for other mixed data types."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, all numeric columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion of the data types can be done using the `astype` method. For example, let us convert the `survived` feature (containing 0 and 1) to `bool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# convert the survived feature to bool\n",
    "df['survived'] = df['survived'].astype(bool)\n",
    "\n",
    "# check the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, we can see that the `survived` feature is now of type `bool`, i.e., `True` and `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other transformations are natural such as converting the `embarked` and `sex` feature to `category`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embarked_category'] = df['embarked'].astype('category')\n",
    "df['sex_category'] = df['sex'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not change the values of the feature, but it changes the data type, the `pclass` feature is now of type `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['embarked_category', 'sex_category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pcalss` feature can also be considered as a categorical feature. Let us convert it to `category` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pclass_category'] = df['pclass'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the `category` data type vs. `object` data type (optional)\n",
    "\n",
    "This is useful for performance reasons, as in some cases, the `category` data type is more efficient than `object` data type. For example, let us compare the performance of the `category` and `object` data types when counting the number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit_cat_vs_other(df, feature):\n",
    "    print()\n",
    "    feature_category = f'{feature}_category'\n",
    "    print(f'Time to count the number of unique values in the {feature} feature (dtype = {df[feature].dtype}) vs. {df[feature_category].dtype})')\n",
    "    %timeit df[feature].nunique()\n",
    "    %timeit df[feature_category].nunique()\n",
    "\n",
    "timeit_cat_vs_other(df, 'sex')\n",
    "timeit_cat_vs_other(df, 'embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in terms of memory usage, the `category` data type is more efficient than `object` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['embarked', 'embarked_category', 'sex', 'sex_category', 'pclass', 'pclass_category']].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations of categorical columns can also have significant impact on the performance of some methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to convert the sex feature to upper case')\n",
    "%timeit df['sex'].str.upper()\n",
    "%timeit df['sex_category'].str.upper()\n",
    "\n",
    "print()\n",
    "print('Time to convert the embarked feature to upper case')\n",
    "%timeit df['embarked'].str.upper()\n",
    "%timeit df['embarked_category'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other operations the `category` data type is **not** more efficient than `object` data type. For example, let us compare the performance of the `category`, `object` and `int64` a data types when counting the number of occurrences of each value. \n",
    "\n",
    "This is because the `category` data type is stored as an array of integers, and the `object` data type is stored as an array of pointers to the strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to count the number of occurrences of each value in the embarked feature')\n",
    "%timeit df['embarked'].value_counts()\n",
    "%timeit df['embarked_category'].value_counts()\n",
    "\n",
    "print()\n",
    "print('Time to count the number of occurrences of each value in the pclass feature')\n",
    "%timeit df['pclass'].value_counts()\n",
    "%timeit df['pclass_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or when doing a group by operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Time to group by the embarked feature')\n",
    "%timeit df.groupby('embarked').size()\n",
    "%timeit df.groupby('embarked_category').size()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('Time to group by pclass feature')\n",
    "%timeit df.groupby('pclass').size()\n",
    "%timeit df.groupby('pclass_category').size()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the conversion of the data types can have significant impact on the performance. In general, the `category` data type is more efficient than `object` data type, but not always. This must be evaluated on a case by case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding\n",
    "Feature encoding is the process of transforming the features to have a more machine learning friendly format. For example, categorical features are transformed to have integer values. This is useful for some machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding is the process of transforming the features to have a more machine learning friendly format. For example, categorical features are split into multiple binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_titanic.copy()\n",
    "\n",
    "encoded_data = pd.get_dummies(df[['embarked', 'sex']],\n",
    "                              dtype=int # default is bool\n",
    ")\n",
    "\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the encoded data to the original data frame and drop the original features\n",
    "df = pd.concat([df, encoded_data], axis=1).drop(['embarked', 'sex'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "Label encoding is the process of transforming the features to, usually, have a more machine learning friendly format. For example, categorical features are transformed to have integer values.\n",
    "\n",
    "Let us encode the `embarked` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# encode the embarked feature\n",
    "le = LabelEncoder()\n",
    "df['embarked'] = le.fit_transform(df['embarked'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LabelEncoder` class has a `classes_` attribute that contains the list of classes that were encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to `LabelEncoder`, for categorial features we can use the `cat.codes` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_titanic.copy()\n",
    "df['embarked'] = df['embarked'].astype('category')\n",
    "df['embarked'] = df['embarked'].cat.codes\n",
    "df['embarked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal encoding\n",
    "\n",
    "Ordinal encoding is similar to label encoding, but the classes are ordered. For example, the classes `low`, `medium`, and `high` can be encoded as `0`, `1`, and `2`, respectively.\n",
    "\n",
    "In the Titanic dataset, we can consider the `pclass` feature as an ordinal feature.   "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_titanic.pclass"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "In practical scenarios, features often have distinct ranges, magnitudes, and units. For instance, age may vary between 0 and 120, while salary can fluctuate between zero and thousands or even millions. This raises the question of how data analysts or scientists can compare such features, given that they are on different scales. It is worth noting that high-magnitude features tend to have a more significant impact on machine learning models than lower magnitude ones. Fortunately, feature scaling or normalization can help address these issues.\n",
    "\n",
    "So, feature scaling refers to the process of bringing all features to the same magnitude level. It is not mandatory for all algorithms, but some algorithms necessitate scaled data, such as those that depend on Euclidean distance measures, like K-nearest neighbor and K-means clustering algorithms.\n",
    "\n",
    "Scalllin can also be used to annoniymize the data. For example, the age of a person can be scaled to the range [0, 1] by dividing by 100. This way, the age of a person is not directly available in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard scaling\n",
    "Standard scaling is the process of transforming the features to have a more normal distribution. The standard scaling is performed by subtracting the mean and dividing by the standard deviation., i.e., $$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# standard scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[features])\n",
    "\n",
    "\n",
    "df[features] = scaler.transform(df[features])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us see the distribution of the original data and the scaled data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=10, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=10, title='standard scaled data', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of the original data and the scaled data is different. The scaled data has a mean of zero and a standard deviation of one. The scaled data is centered around zero, and the values are almost all within the range of -3 and 3. The scaled data is more normally distributed than the original data.\n",
    "\n",
    "However, plotting the data in a scatter plot, we can see that the shape of the data is the same, only the scale is different."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ax = df_titanic['age'].sort_values().reset_index().drop('index', axis=1).plot(style='o', title='original data')\n",
    "df['age'].sort_values().reset_index().drop('index', axis=1).plot(style=\".\",ax=ax, secondary_y=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max scaling\n",
    "\n",
    "Min-max scaling is the process of transforming the features to have a more uniform distribution. The min-max scaling to the [0, 1] interval is performed by subtracting the minimum and dividing by the range., i.e., $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}.$$\n",
    "\n",
    "To transform the data to the $[a, b]$ interval, we can use the following formula: $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}  (b - a) + a.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us see the distribution of the original data and the scaled data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='min-max scaled data', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of the original data and the scaled data is different. The scaled data is in the [0, 1] interval. The scaled data is more uniformly distributed than the original data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust scaling\n",
    "Robust scaling features in a way that accounts for outliers. The method achieves this by first removing the median and then scaling the data based on the quantile range. The default quantile range used is the Interquartile Range (IQR), although it can be customized if needed.\n",
    "\n",
    "During the scaling process, each feature is centered and scaled independently by computing relevant statistics from the training set. Outliers can often skew the sample mean and variance in undesirable ways.\n",
    "\n",
    "So, robust scaling is computed as follows:\n",
    " $$x_{scaled} = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)},$$\n",
    "where $\\text{median}(x)$ is the median of the feature $x$, and $\\text{IQR}(x)$ is the interquartile range of the feature $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# robust scaling\n",
    "scaler = RobustScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us see the distribution of the original data and the scaled data. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='robust scaled data', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of the original data and the scaled data is different. The scaled data is centered around zero and is more uniformly distributed than the original data. The scaled data is more robust to outliers than the original data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "Feature transformation is the process of transforming the features allows, for exameple, reducing the skewness of the features, the effect of outliers, etc. Examples feature transformations are log transformation, square root transformation, square, etc.\n",
    "\n",
    "If the feature is right-skewed or positively skewed or grouped at lower values, then we can apply the square root, cube root, and logarithmic transformations, while if the feature is left-skewed or negative skewed or grouped at higher values, then we can apply the cube, square, and so on.\n",
    "\n",
    "### Log transformation\n",
    "\n",
    "The log transformation is performed by taking the logarithm of the feature., i.e., $$x_{log} = \\log(x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# log transformation\n",
    "df[features] = df[features].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='log scaled data', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square root transformation\n",
    "Square root transformation is performed by taking the square root of the feature., i.e., $$x_{sqrt} = \\sqrt{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# square transformation\n",
    "df[features] = df[features].apply(lambda x: np.sqrt(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "df_titanic[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='original data', ax=ax[0])\n",
    "df[['age', 'fare']].plot.hist(alpha=0.5, bins=20, title='log scaled data', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization transformation\n",
    "Discretization transformation is performed by transforming numerical features to categorical features. This is useful for some machine learning algorithms. For example, the following code transforms the age feature to a categorical feature.\n",
    "\n",
    "To do this, we use the `pd.cut` function. The `pd.cut` function takes as input the feature to be transformed, the bins, and the labels. The bins are the intervals in which the feature will be transformed. The labels are the names of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['age', 'fare']\n",
    "\n",
    "# discretization transformation\n",
    "df['age category'] = pd.cut(df['age'], bins=[0, 18, 30, 65, 100], labels=['child', 'young', 'adult', 'senior'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age', 'age category']].groupby('age category', observed=True).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature splitting\n",
    "Feature splitting is the process of splitting a feature into multiple features. For example, some times is possible to split the `name` feature into two features: `first name` and `last name`. Or the spliting of a `date` feature into three features: `year`, `month`, and `day`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dates example\n",
    "\n",
    "Consider the dataset with energy consumption of a house. The dataset contains the date and the energy consumption of the house. The date feature can be split into multiple features, such as year, month, day, hour, minute, and second. This can be done using the `dt` accessor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_consumption = pd.read_csv('./data/house_consumption_TS/house_consumption.csv')\n",
    "df_consumption.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_consumption.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_consumption['date'] = pd.to_datetime(df_consumption['date'])\n",
    "df_consumption.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_consumption['year'] = df_consumption['date'].dt.year\n",
    "df_consumption['month'] = df_consumption['date'].dt.month\n",
    "df_consumption['day'] = df_consumption['date'].dt.day\n",
    "df_consumption['hour'] = df_consumption['date'].dt.hour\n",
    "df_consumption['minute'] = df_consumption['date'].dt.minute\n",
    "df_consumption['second'] = df_consumption['date'].dt.second\n",
    "df_consumption"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Titanic example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Titanic dataset, we can split the cabin feature into two features: `cabin number` and `cabin letter`. The latter corresponds to the deck of the Titanic. Furhter, some passenger have more than one cabin. In this case, we can split the cabin feature into multiple features, one for each cabin."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `extract` function can be used to extract the deck from the cabin feature. The `extract` function takes as input a regular expression that defines the pattern to be extracted. For example, the following code extracts the deck from the cabin feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['cabin']\n",
    "\n",
    "# feature splitting\n",
    "df['cabin number'] = df['cabin'].str.extract('([0-9]+)')\n",
    "df['deck'] = df['cabin'].str.extract('([A-Z])')\n",
    "df['number of cabins'] = df['cabin'].str.split().str.len()\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it can be interesting to see the distribution of the passenger class in different decks. This can be done using the `groupby` function and the `unstack` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pclass', 'deck']]\\\n",
    "    .groupby('deck').value_counts()\\\n",
    "    .unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pclass', 'deck']] \\\n",
    "    .groupby('deck').value_counts() \\\n",
    "    .unstack()\\\n",
    "    .plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can see the distribution of the passenger class in different decks using the `pivot_table` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='deck', columns='pclass', aggfunc='size', fill_value=0).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can check is the correlation between the number of cabins and the passenger class or the fare. We can see that the number of cabins is highly correlated fare but not with the passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['number of cabins', 'pclass', 'fare']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the distribution of the mean fare in different passenger classes and number of cabins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='pclass', columns='number of cabins', values='fare', aggfunc='size', fill_value=0)#.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression in a nutshell\n",
    "\n",
    "Regular expressions are a powerful tool for string manipulation. They are used to find, replace, and split strings. They are also used to extract information from strings. For example, we can extract the title of the passenger from the name feature (as we'll see later).\n",
    "\n",
    "So, a regular expression (regex) is a pattern used to match character combinations in strings. Here's a nutshell tutorial on regular expressions:\n",
    "- **Matching characters**: You can match specific characters by simply including them in the pattern. For example, the pattern \"cat\" matches the characters \"c\", \"a\", and \"t\".\n",
    "\n",
    "- **Character classes**: You can match a set of characters by using character classes. For example, the pattern [abc] matches any of the characters \"a\", \"b\", or \"c\".\n",
    "\n",
    "- **Metacharacters**: Certain characters have a special meaning in regex patterns. For example, the dot \".\" matches any character except a newline, and the asterisk \"*\" matches zero or more occurrences of the previous character.\n",
    "\n",
    "- **Anchors**: Anchors are used to match the position of a string. The caret \"^\" matches the start of a string, and the dollar sign \"\\$\" matches the end of a string.\n",
    "\n",
    "- **Quantifiers**: Quantifiers are used to specify how many times a character or group of characters should appear. For example, the pattern a{3} matches exactly three occurrences of the letter \"a\", and the pattern a{3,5} matches three to five occurrences of the letter \"a\".\n",
    "\n",
    "- **Alternation**: Alternation is used to match one of several possible patterns. For example, the pattern cat|dog matches either \"cat\" or \"dog\".\n",
    "\n",
    "- **Grouping**: Grouping is used to group parts of a pattern together. This is useful for applying quantifiers or alternation to a group of characters. For example, the pattern (ab)+ matches one or more occurrences of the string \"ab\".\n",
    "\n",
    "- \"\\d\" is a metacharacter that represents any digit from 0 to 9. It is commonly used to match numbers in text, and it is equivalent to the character class [0-9].\n",
    "\n",
    "- [a-z] is a character class that matches any lowercase letter from \"a\" to \"z\". It represents a range of characters between \"a\" and \"z\" inclusive. For example, the pattern c[a-z]t matches any three-letter word that starts with \"c\" and ends with \"t\", where the middle letter can be any lowercase letter. This would match words such as \"cat\", \"cet\", \"cxt\", and so on.\n",
    "\n",
    "- Some more examples: \n",
    "    -The pattern [a-z]+ matches one or more consecutive lowercase letters;\n",
    "\n",
    "    - The pattern [a-z]{3} matches exactly three consecutive lowercase letters;\n",
    "\n",
    "    - The pattern ^[a-z]+$ matches a string that consists entirely of lowercase letters;\n",
    "\n",
    "    - You can also use other character classes in regular expressions to match other types of characters, such as uppercase letters ([A-Z]), digits (\\d), whitespace (\\s), or non-word characters (\\W).\n",
    "\n",
    "See https://docs.python.org/3/library/re.html for more information on the Python regular expression module. See also (Campesato, 2018).\n",
    "\n",
    "Test your regular expressions using https://regex101.com/ or https://regexr.com/\n",
    "\n",
    "Let me give you an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "name = 'Braund, Mr. Owen Harris & Bradley, Mrs. Anna Michaela'\n",
    "\n",
    "def print_regex_result(explanation, name, regex):\n",
    "    print(re.findall(regex, name), \":\", explanation)\n",
    "\n",
    "print_regex_result('title, extracted considering the dot ', name, '([A-Za-z]+)\\.')\n",
    "print_regex_result('title, extracted considering the dot ', name, '(\\w+)\\.')\n",
    "print_regex_result('title, extracted considering the comma', name, ',\\s([A-Za-z]+)')\n",
    "print_regex_result('last name, extracted considering the comma', name, '([A-Za-z]+),')\n",
    "print_regex_result( 'first name, extracted considering the dot', name, '\\.\\s([A-Za-zç]+)')\n",
    "print_regex_result('first name and posterior name, extracted considering the space between words', name, '([A-Za-zç]+)\\s([A-Za-z]+)')\n",
    "\n",
    "print_regex_result('list of words', name, '([A-Za-z]+)')\n",
    "print_regex_result('list of words', name, '(\\w+)')\n",
    "print_regex_result('list of names', name, '([A-Za-z]+),\\s\\w+.\\s([A-Za-z]+)\\s([A-Za-z]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Feature engineering includes\n",
    "- the creation of new features from the existing features, \n",
    "- the selection of features, \n",
    "- the extraction of features, \n",
    "- the reduction of features, and \n",
    "- the aggregation of features. \n",
    "                \n",
    "We already saw how to create new features from the existing features, by extracting the deck from the cabin feature.\n",
    "\n",
    "As another example, we can create a new columns with the title and last name of the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = df_titanic.copy()\n",
    "\n",
    "# select the features\n",
    "features = ['name']\n",
    "\n",
    "# feature engineering\n",
    "df['title'] = df['name'].str.extract('([A-Za-z]+)\\.')\n",
    "df['last name'] = df['name'].str.extract('([A-Za-z]+),')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "### Correlation as a feature selection technique\n",
    "In feature selection we select the features that are useful for the analysis. We can select the features using the correlation with the target variable. For example, we can select the features that have a correlation with the target variable greater than 0.1 to predict the survival of the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the numerical features\n",
    "df = df_titanic.select_dtypes(include=np.number)\n",
    "\n",
    "# compute the correlation with the target variable\n",
    "df.corr()['survived'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using the variance\n",
    "We can also select the features using the variance. For example, we can select the features that have a variance greater than 0.1 to predict the survival of the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the numerical features\n",
    "df = df_titanic.select_dtypes(include=np.number)\n",
    "\n",
    "# compute the variance\n",
    "df.var().sort_values(ascending=False)\n",
    "\n",
    "# select the features with variance greater than 0.1\n",
    "df = df.loc[:, df.var() > 0.1]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Campesato, O. (2018). Regular expressions: Pocket primer. Mercury Learning and Information.\n",
    "- https://www.kaggle.com/learn/pandas\n",
    "- Navlani, A.,  Fandango, A.,  Idris, I. (2021). Python Data Analysis: Perform data collection, data processing, wrangling, visualization, and model building using Python. Packt. 3rd Edition\n",
    "- Brandt. S. (2014). Data Analysis: Statistical and Computational Methods for Scientists and Engineers. Springer. 4th Edition\n",
    "- https://eugenelohh.medium.com/data-analysis-on-the-titanic-dataset-using-python-7593633135f2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
