{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exploratory analysis of TS data\n",
    "\n",
    "## Introduction\n",
    "I **time series (TS)** data is a sequence of data points collected at regular intervals over time. Time series data is commonly used in a wide range of applications, such as finance, economics, weather forecasting, and many others. Analyzing time series data can help identify trends, patterns, and relationships in the data, as well as make predictions about future values.\n",
    "\n",
    "For example\n",
    "- **Stock market data**: Stock prices are collected at regular intervals, such as daily or hourly, and can be used to analyze trends in the stock market and make predictions about future prices.\n",
    "- **Weather data**: Weather data is collected at regular intervals, such as hourly or daily, and can be used to analyze patterns in temperature, precipitation, and other weather variables.\n",
    "- **Building energy consumption**: Energy consumption data is collected at regular intervals, such as hourly or daily, and can be used to analyze patterns in energy usage and make predictions about future consumption.\n",
    "- etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to explore time series data?\n",
    "\n",
    "There are several ways to explore a time series, including:\n",
    "\n",
    "1. **Plotting the time series**: A simple line plot of the data over time can provide insights into trends, seasonality, and other patterns in the data.\n",
    "2. **Decomposition**: Decomposing a time series into its **trend, seasonal, and residual components** can help identify underlying patterns and trends.\n",
    "3. **Autocorrelation**: Autocorrelation measures the **correlation between a time series and a lagged version of itself**. Examining autocorrelation plots can help identify patterns such as seasonality or random noise.\n",
    "4. **Time series forecasting**: Using statistical or machine learning models to forecast future values of the time series can help identify potential future trends or patterns.\n",
    "5. **Spectral analysis**: Spectral analysis can help identify the frequency components of a time series, which can be useful in identifying seasonal or cyclical patterns.\n",
    "6. **Clustering and classification**: Clustering and classification techniques can be used to group similar time series together or classify them based on certain characteristics, which can provide insights into patterns or trends across multiple time series.\n",
    "7. **Time series anomaly detection**: Identifying outliers or anomalies in a time series data can provide insights into unusual patterns or events that might be affecting the series.\n",
    "8. **Correlation analysis**: Correlation analysis can be used to examine the **relationship between a time series and other variables**, such as economic indicators or weather data.\n",
    "9. **Time series smoothing**: Smoothing techniques such as **moving averages or exponential smoothing** can help reduce noise and highlight underlying patterns in the data.\n",
    "10. **Time series feature extraction**: Feature extraction techniques can be used to identify relevant features in a time series that can be used for prediction or classification purposes.\n",
    "11. **Granger causality analysis**: Granger causality analysis can help identify the **causal relationships between multiple time series**, which can be useful in identifying important drivers of trends or patterns.\n",
    "12. **Event detection**: Event detection can help identify specific events that might be affecting a time series, such as major news events or policy changes\n",
    "\n",
    "Overall, exploring a time series involves a combination of visualizations, statistical analyses, and modeling techniques to identify patterns and trends in the data, and to uncover potential drivers of those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Time Series\n",
    "\n",
    "There are several libraries available in Python for plotting time series data, some of the popular libraries are:\n",
    "\n",
    "1. **Matplotlib**: Matplotlib is a powerful plotting library for Python. It provides several functions for creating line plots, scatter plots, bar charts, and other types of visualizations. Matplotlib is the foundation for many other plotting libraries in Python.\n",
    "1. **Seaborn**: Seaborn is a data visualization library that builds on top of Matplotlib. It provides functions for creating more complex plots, such as heatmaps, pair plots, and violin plots. Seaborn is particularly useful for visualizing statistical relationships in data.\n",
    "1. **Plotly**: Plotly is an interactive visualization library that can be used to create dynamic and interactive plots, such as line plots, scatter plots, and heatmaps. Plotly allows for interactive zooming and panning, as well as hover effects and other features.\n",
    "1. **Bokeh**: Bokeh is another interactive visualization library for Python. It provides tools for creating interactive and zoomable plots, as well as widgets for controlling the visualization. Bokeh is particularly useful for creating web-based dashboards and other interactive visualizations.\n",
    "1. **Pandas**: Pandas is a powerful data analysis library in Python that provides several functions for working with time series data. Pandas has built-in functions for creating line plots, scatter plots, and other types of visualizations, as well as time series-specific functions such as rolling means and resampling.\n",
    "\n",
    "These libraries provide several options for visualizing time series data in Python, and the choice of library depends on the specific needs of the analysis and the preferences of the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several types of plots that can be used to visualize time series data, some of the most common ones include:\n",
    "\n",
    "### Line plot\n",
    "A line plot is a basic plot that shows how the value of a variable changes over time. It is a good choice for visualizing trends in the data over time, as well as identifying seasonal or cyclical patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install calplot\n",
    "# !pip install mplfinance\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calplot\n",
    "import mplfinance as mpf\n",
    "import pandas_bokeh\n",
    "import numpy as np\n",
    "\n",
    "# set pandas_bokeh to output inline\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the stock market data which was downloaded from the [Yahoo Finance](https://finance.yahoo.com/) website. The used script will be available in the [data folder](./data/stock_market/download_data.ipynb) and allow you to update the data. However, note that updating the data will change the results of the plots and consequently the results of the analysis.\n",
    "\n",
    "The data is stored in a csv file, which can be loaded into a pandas dataframe. \n",
    "\n",
    "Note the `parse_dates=True` argument, which tells pandas to parse the 'Date' column as a datetime object. This allows us to easily work with the dates in the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_stocks = pd.read_csv('./data/stock_market/stocks.csv', \n",
    "                        index_col='Date', \n",
    "                        parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stocks.dropna(inplace=True)\n",
    "df_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each stock there is the open, high, low, close, and volume values. The scocks considered are AAPL, AMZN, GOOGL, META, TSLA, and MSFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stocks =  ['AAPL', 'AMZN', 'GOOGL', 'META', 'TSLA', 'MSFT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For our first example, we'll only plot the close values, using the plot function of **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the 'Close' columns\n",
    "close_cols = [col for col in df_stocks.columns if 'Close' in col]\n",
    "\n",
    "# plot the closing values\n",
    "_ = df_stocks[close_cols].plot(figsize=(20,8),\n",
    "                               title='Stocks closing values',\n",
    "                               xlabel='Date',\n",
    "                               ylabel='Close price',\n",
    "                               grid=True,\n",
    "                               legend=True,\n",
    "                               colormap='tab10',\n",
    "                               alpha=0.8,\n",
    "                               linewidth=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we can use **seaborn** to plot the closing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the closing values\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.lineplot(data=df_stocks[close_cols], \n",
    "                 dashes=False,\n",
    "                 palette='tab10',\n",
    "                 linewidth=0.5,\n",
    "                 alpha=0.8,\n",
    "                 markers=False)\n",
    "ax.set_title('Stocks closing values')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Close price')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the bokeh library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df_stocks[close_cols].plot_bokeh(\n",
    "    figsize=(1200,600),\n",
    "    legend = \"top_left\",\n",
    "    ylabel = \"Close price\",\n",
    "    xlabel = \"Date\",\n",
    "    title = \"Stocks closing values\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other variation can also be plotted as the percentage of variation of the stocks in predetermined periods.\n",
    "\n",
    "For this, we do a resample of the data to aggregate the values in the desired periods. For example, we can aggregate the max closing values of the AAPL stock in 15 days periods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the data to get the max closing values in 15 days periods\n",
    "df_stocks['Close_AAPL'].resample('15D').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the variation of the stocks in different periods, we can use the aggregation and plot functions of pandas. Let's first define a function to compute the variation of the stocks in the given period. See the possible intervals in https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variation(df, aggr_flag):\n",
    "    \"\"\"\n",
    "    Compute the variation of the stock in the given period\n",
    "    \"\"\"\n",
    "    # get the close and open columns\n",
    "    close_cols = [col for col in df.columns if 'Close' in col]\n",
    "    close_cols.sort()\n",
    "\n",
    "    open_cols = [col for col in df.columns if 'Open' in col]\n",
    "    open_cols.sort()\n",
    "\n",
    "    # close cols without suffix\n",
    "    cols = [col.split('_')[1] for col in close_cols]\n",
    "\n",
    "    # compute the period's opening values\n",
    "    open_df = df[open_cols].resample(aggr_flag).first()\n",
    "    open_df.columns = cols\n",
    "\n",
    "    # compute the period's closing values\n",
    "    close_df = df[close_cols].resample(aggr_flag).last()\n",
    "    close_df.columns = cols\n",
    "\n",
    "    return close_df - open_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the variation of the stocks\n",
    "list_of_aggr = ['W', '15D', 'ME', '3ME', '6ME', 'YE', '5YE']\n",
    "list_of_msg = ['weekly', '15 days', 'monthly', '3 months', '6 months', 'yearly', '5 years']\n",
    "\n",
    "n_plots = len(list_of_aggr)\n",
    "\n",
    "fig, ax = plt.subplots(n_plots, figsize=(20,40))\n",
    "\n",
    "for i, aggr_flag, msg in zip(range(n_plots), list_of_aggr, list_of_msg):\n",
    "    delta_for_period = compute_variation(df_stocks, aggr_flag)\n",
    "    delta_for_period.plot(ax=ax[i], title=f\"stock maximum value variation on {msg} aggregation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "A scatter plot is a plot that shows the relationship between two variables. It can be useful for identifying relationships between variables in time series data, as well as identifying outliers or anomalies.\n",
    "\n",
    "For example, you can see the obvious relationship between opening and closing of the AAPL stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(x=df_stocks['Close_AAPL'], y=df_stocks['Open_AAPL'], s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " And below a scatter plot with openings of AAPL and GOOGL, which show also a correlation between the opening values  of the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(x=df_stocks['Open_AAPL'], y=df_stocks['Open_GOOGL'], s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix of scatter plots can also be made in a simple manner by using the pandas scatter_matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.plotting.scatter_matrix(df_stocks[close_cols], \n",
    "                               figsize=(20,20),\n",
    "                               diagonal='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can also plot the daily valorization (close - opening) of the stocks. The diagonal shows an the histogram of the valorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the valorization\n",
    "daily_delta = compute_variation(df_stocks, 'D')\n",
    "\n",
    "_ = pd.plotting.scatter_matrix(daily_delta, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar chart\n",
    "A bar chart is a plot that shows the frequency of a categorical variable. It can be used to visualize the frequency of events that occur over time, such as the number of sales in a given month or the number of website visits in a given week.\n",
    "\n",
    "For example, we can plot yearly valorization of the stocks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the data yearly aggregated\n",
    "# to simplify, we only consider the closing of the last and first day of the year\n",
    "df_delta_year = compute_variation(df_stocks, 'YE')\n",
    "df_delta_year.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_delta_year.index = df_delta_year.index.year\n",
    "df_delta_year.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "ax = df_delta_year.plot(kind=\"bar\",figsize=(20, 10), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap\n",
    "A heatmap is a plot that shows the distribution of values in a two-dimensional grid. It can be used to visualize the relationship between two variables over time, such as the temperature and humidity in a given location over the course of a year.\n",
    "\n",
    "For instance a plot of the correlation between daily stocks valorization can be done on the following way (TSLA is the one less correlated with the others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta_daily = compute_variation(df_stocks, 'D')\n",
    "corr_matrix = df_delta_daily.corr()\n",
    "sns.heatmap(corr_matrix)\n",
    "corr_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of time series, it can plot the values against time. For example de variation of value GOOGL over 2021 and 2022 can be plotted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the calplot library is needed - see https://calplot.readthedocs.io/en/latest/\n",
    "data = df_delta_daily['GOOGL'].loc[df_delta_daily.index.year.isin([2018, 2019, 2020, 2021, 2022])]\n",
    "calplot.calplot(data, \n",
    "                cmap=\"coolwarm\", \n",
    "                colorbar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot\n",
    "A box plot is a plot that shows the distribution of a variable over time. It can be useful for identifying trends in the data, as well as outliers or anomalies.\n",
    "\n",
    "In more detail, The box plot consists of a box and whiskers that represent the distribution of the data. The box represents the middle 50% of the data, with the bottom of the box indicating the 25th percentile and the top indicating the 75th percentile. The whiskers extend from the box to the minimum and maximum values.\n",
    "\n",
    "To identify outliers, the interquartile range (IQR) is used. The interquartile range (IQR) is the distance between the 25th (Q1) and 75th (Q3) percentiles of the data. You can compute the IQR by subtracting the 25th percentile from the 75th percentile. Then outliers are points that fall outside the whiskers. The whiskers extend to 1.5 times the IQR below the 25th percentile and above the 75th percentile.\n",
    "\n",
    "For example, we can plot the daily variation of the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_delta_daily.boxplot(grid=True)\n",
    "_ = ax.set_title(\"Stocks' daily variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above scales is hard to get an idea of the real values. But we can see more details asking for a description of the framework (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta_daily.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the monthly and yearly variations. Observe that the number of samples for the yearly variation is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta_month = compute_variation(df_stocks, 'ME')\n",
    "ax = df_delta_month.boxplot(grid=True)\n",
    "_ = ax.set_title(\"Stocks' monthly variation\")\n",
    "\n",
    "df_delta_month.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta_year = compute_variation(df_stocks, 'YE')\n",
    "ax = df_delta_year.boxplot(grid=True)\n",
    "_ = ax.set_title(\"Stocks' yearly variation\")\n",
    "\n",
    "df_delta_year.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "A histogram is a plot that shows the distribution of a variable over time. It can be useful for identifying the frequency of events that occur at different points in time, such as the number of website visits during different times of the day.\n",
    "\n",
    "In the following example, the histogram of the daily variations of AAPL is shown. The number of bins is the number of intervals the values interval is divided into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = compute_variation(df_stocks, 'D')\n",
    "_ = df_delta['AAPL'].plot(kind='hist', alpha=0.5, bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the histogram of the quarters variations of AAPL is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = compute_variation(df_stocks, 'QE')\n",
    "_ = df_delta['AAPL'].plot(kind='hist', alpha=0.5, bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area plot\n",
    " An area plot is a plot that shows the area under a line plot. It can be used to visualize the total value of a variable over time, as well as the distribution of the variable over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_stocks[close_cols].plot.area(stacked=False, alpha=0.2, figsize=(20,8))\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Area Plot of the stocks value')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked area plot\n",
    "A stacked area plot is a plot that shows the distribution of multiple variables over time. It can be useful for visualizing the total value of multiple variables over time, as well as the relative contribution of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_stocks[close_cols].plot.area(stacked=True, alpha=0.5, figsize=(20,8))\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Area Plot of the stocks value')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candlestick chart\n",
    "A candlestick chart is a plot that shows the open, close, high, and low values of a variable over time. It can be useful for visualizing the price of a security or commodity over time, as well as identifying trends and patterns in the data.\n",
    "\n",
    "In this case, the data has to be in the format of the mplfinance package (see https://pypi.org/project/mplfinance/ for more details), that is, a dataframe with columns Open, High, Low, Close, and optionally Volume and Datetime.\n",
    "\n",
    "Its plot presents the daily variation and can be complemented with moving averages and volume, as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for the last 120 days\n",
    "n_days = 120\n",
    "apple_cols = [col for col in df_stocks.columns if 'AAPL' in col]\n",
    "df_AAPL = df_stocks.iloc[-n_days:,:][apple_cols]\n",
    "\n",
    "# correct the name of the columns, so the mplfinance package  understands them\n",
    "df_AAPL.columns = [col.split('_')[0] for col in df_AAPL.columns]\n",
    "\n",
    "# plot\n",
    "mpf.plot(df_AAPL,\n",
    "         mav=(2, 7, 15), # moving averages of 2, 7, and 15 days\n",
    "         volume=True,\n",
    "         figsize=(20,10),\n",
    "         type='candle', # possible types are: 'candle', 'ohlc', 'line', 'renko', 'pnf'\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining plots\n",
    "Combining plots is a useful technique in data visualization. Combining allow to compare multiple datasets, helping to visualize multiple datasets in the same plot, making it easier to compare them visually. For example, you can plot two or more lines on the same plot to compare the trend of multiple variables over time. It also allows to show different aspects of data, since combining different types of plots, such as scatter plots, bar charts, and histograms, can help you to show different aspects of your data. For example, you can use a scatter plot to show the relationship between two variables and a histogram to show the distribution of a single variable.\n",
    "\n",
    "Let us see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = compute_variation(df_stocks, 'D')\n",
    "\n",
    "# define the pairplot\n",
    "g = sns.PairGrid(df_delta)\n",
    "\n",
    "# define the plots for the diagonal, lower, and upper triangles\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_upper(sns.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform time series\n",
    "\n",
    "Transform TS data for modeling allows to transform, rescale, smooth, and engineer features from time series data in order to best expose the underlying inherent properties of the problem (the signal) to forecasting learning algorithms.\n",
    "\n",
    "Some examples are\n",
    "- transforming\n",
    "- rescaling\n",
    "- smoothing\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "Converting the data into a more suitable format, such as logarithmic or exponential transformations, to make the data conform more closely to certain statistical assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data between 2014 and 2022\n",
    "df_close_14_22 = df_stocks.loc['2014-01-01':'2022-12-31', close_cols]\n",
    "df_close_log = df_close_14_22.map(np.log)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, sharex=True, figsize=(20,8))\n",
    "\n",
    "# plot of the original data\n",
    "_ = df_close_14_22.plot(ax=ax[0], title='original data')\n",
    "\n",
    "# plot the log data\n",
    "_ = df_close_log.plot(ax=ax[1], title='log data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling\n",
    "\n",
    "Scaling the data to a common range, such as between 0 and 1 or using standardization, can help to compare and interpret features across different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_close_14_22),\n",
    "                         index=df_close_14_22.index,\n",
    "                         columns=df_close_14_22.columns)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, sharex=True, figsize=(20,12))\n",
    "\n",
    "_ = df_close_14_22.plot(ax=ax[0], title='original data')\n",
    "_ = df_scaled.plot(ax=ax[1], title='Min-max scalled data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume ts_data is a numpy array containing the original time series data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_close_14_22),\n",
    "                         index=df_close_14_22.index,\n",
    "                         columns=df_close_14_22.columns\n",
    "                         )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, sharex=True, figsize=(20,12))\n",
    "\n",
    "_ = df_close_14_22.plot(ax=ax[0], title='original data')\n",
    "_ = df_scaled.plot(ax=ax[1], title='Standard scalled data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series smoothing\n",
    "\n",
    "Time series smoothing is a technique used to remove short-term fluctuations or noise from a time series data in order to better identify the long-term trends and patterns. The objective of time series smoothing is to improve the signal-to-noise ratio in the data by eliminating the random variations that make it difficult to detect the underlying trend or pattern.\n",
    "\n",
    "\n",
    "Overall, time series smoothing can help to improve the accuracy of forecasting and prediction models by **reducing the effects of noise and random variations in the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series smoothing can be used to remove different types of variations from the data, such as:\n",
    "- **Seasonal variation**: Seasonal variation refers to the patterns that repeat periodically over time. For example, sales of air conditioners may increase during summer months and decrease during winter months.\n",
    "\n",
    "- **Cyclical variation**: Cyclical variation refers to the patterns that repeat at irregular intervals. For example, the stock market may experience booms and busts over several years.\n",
    "\n",
    "- **Random variation/noise**: Random variation refers to the unpredictable fluctuations in the data that are not related to any specific trend or pattern.\n",
    "\n",
    "There are different techniques that can be used for time series smoothing, such as:\n",
    "\n",
    "- **Moving average smoothing**\n",
    "- **Exponential smoothing**\n",
    "- **Seasonal decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving average smoothing\n",
    "**Moving average smoothing**  is used to help reveal the trend of a time series dataset without all the noise. It is calculated by taking the average of the dataset over a sliding window of time. The window is specified by the width of the window in terms of the number of observations. For example, a window width of 3 would be a moving average calculated with 3 observations at a time. The moving average is calculated as the average of the observations in the window, where the window slides one observation at a time. This means that the first few moving average values are not a number until the window has observations to work with.\n",
    "\n",
    "The values are calculated using the following formula:\n",
    "$$ \\frac{1}{n} \\sum_{i=0}^{n-1} x_{t-i} $$\n",
    "where $n$ is the window width and $x_{i}$ is the observation at time $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window size\n",
    "The window size in the moving average is an important parameter that determines the level of smoothing applied to the time series data. Generally, a larger window size results in a smoother time series with less detail and a smaller window size results in a more detailed time series with more noise.\n",
    "\n",
    "Choosing an appropriate window size depends on the specific characteristics of the time series data and the goals of the analysis. Some general guidelines for selecting the window size in the moving average are:\n",
    "\n",
    "- Consider the frequency of the data: a smaller window size may be appropriate to capture the short-term fluctuations in the data.  A larger window size may be appropriate to capture the longer-term trends and patterns in the data.\n",
    "\n",
    "- Consider the length of the time series: If the time series data is short, then a smaller window size may be appropriate to avoid oversmoothing the data. If the time series data is long, then a larger window size may be appropriate to smooth out the noise in the data.\n",
    "\n",
    "In the end, it is a good practice to experiment with different window sizes to determine the best value for the specific time series data and analysis goals. For example, you can try different window sizes and compare the resulting smoothed time series using visual inspection.\n",
    "\n",
    "So let us try with other window sizes, which will remove those small peaks in the middle of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_passengers = pd.read_csv('data/passengers_TS/passengers.csv', index_col='Month', parse_dates=True)\\\n",
    "    .drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Apply moving average\n",
    "window_size = 12\n",
    "rolling_mean = df_passengers['#Passengers'].rolling(window_size).mean()\n",
    "\n",
    "# Plot original and smoothed time series data\n",
    "ax = df_passengers.plot(label='Original Data', figsize=(20,8))\n",
    "_ = rolling_mean.plot(ax=ax, label=f'{window_size}-Day Moving Average', title='Moving Average Smoothing example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with distinct window sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(20, 12))\n",
    "\n",
    "for i, window_size in enumerate([3, 6, 9, 12, 12 * 2]):\n",
    "    rolling_mean = df_passengers['#Passengers'].rolling(window_size).mean()\n",
    "\n",
    "    # Plot original and smoothed time series data\n",
    "    axes[i].plot(df_passengers.index, df_passengers['#Passengers'], label='Original Data')\n",
    "    axes[i].plot(rolling_mean.index, rolling_mean, label=f'{window_size}-Month Moving Average')\n",
    "    axes[i].legend()\n",
    "    #axes[i].xlabel('Date')\n",
    "    #axes[i].ylabel('Value')\n",
    "fig.suptitle('Moving Average Smoothing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the sunspots, also some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_sun_spots = pd.read_csv('data/sunspots_TS/sunspots.zip', index_col='Date', parse_dates=True)\\\n",
    "    .drop('Unnamed: 0', axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 12))\n",
    "\n",
    "for i, window_size in enumerate([6, 12, 12 * 11]):\n",
    "    rolling_mean = df_sun_spots['Monthly Mean Total Sunspot Number'].rolling(window_size).mean()\n",
    "\n",
    "    # Plot original and smoothed time series data\n",
    "    df_sun_spots.plot(ax=axes[i], label='Original Data')\n",
    "    rolling_mean.plot(ax=axes[i], label=f'{window_size}-Month Moving Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exponential smoothing\n",
    "**Exponencial smoothing** is used to smooth the moving average values and reduce the noise in the data. It is calculated as a weighted moving average where the weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations.\n",
    "\n",
    "The rate at which they decrease is controlled by a smoothing parameter, called alpha. The alpha parameter is often set to a value between 0 and 1. Larger values mean that recent observations have more weight and therefore more influence on the smoothed value. Smaller values mean that older observations have more weight and can result in predictions that change less radically over time. (see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html)\n",
    "\n",
    "The values are calculated using the following formula:\n",
    "$$ \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "        s_0 = x_0 \\\\\n",
    "        s_t = \\alpha x_t + (1-\\alpha) s_{t-1} \\\\\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "where $x_t$ is the observation at time $t$ and $s_t$ is the smoothed value at time $t$.  In other words, the smoothed statistic $s_t$ is a simple weighted average of the current observation $x_{t}$ and the previous smoothed statistic $s_{t-1}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the smoothed data using a rolling window, with a window size of 15 and 30 days\n",
    "df_mva_15_days = df_close_14_22.rolling(window=15).mean()\n",
    "df_mva_30_days = df_close_14_22.rolling(window=30).mean()\n",
    "\n",
    "# compute the smoothed data using exponential smoothing, with alpha=0.1 and alpha=0.5\n",
    "df_exp_alpha_01 = df_close_14_22.ewm(alpha=0.1).mean()\n",
    "df_exp_alpha_05 = df_close_14_22.ewm(alpha=0.5).mean()\n",
    "\n",
    "fig, ax = plt.subplots(5,1, sharex=True, figsize=(20,20))\n",
    "# plot of the orignal data\n",
    "_ = df_close_14_22.plot(ax=ax[0], title='original data')\n",
    "_ = df_mva_15_days.plot(ax=ax[1], title='smoothed data: 15 days rolling window')\n",
    "_ = df_mva_30_days.plot(ax=ax[2], title='smoothed data: 30 days rolling window')\n",
    "_ = df_exp_alpha_01.plot(ax=ax[3], title='smoothed data: exponential, alpha=0.1')\n",
    "_ = df_exp_alpha_05.plot(ax=ax[4], title='smoothed data: exponential, alpha=0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "A series is said to be **stationary if its statistical properties such as mean, variance remain constant over time**. Most of the Time Series models work on the assumption that the TS is stationary.\n",
    "\n",
    "Intuitively, we can say that if a time series has a particular behaviour over time, there is a very high probability that it will follow the same in the future. Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series.\n",
    "\n",
    "Let us see how to do it using the passengers' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# load time series data\n",
    "df = pd.read_csv('./data/passengers_TS/passengers.csv', index_col='Month', parse_dates=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# plot the data\n",
    "df.plot()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('# pass.')\n",
    "plt.title('Monthly passengers')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can expeculate that, the data is not stationary. There is an increasing trend in the data. We can also see that the mean is not constant over time.\n",
    "\n",
    "We can check stationarity using the following:\n",
    "\n",
    "- **Plotting Rolling Statistics**: We can plot the moving average or moving variance and see if it varies with time. But again this is more of a visual technique.\n",
    "\n",
    "- **Dickey-Fuller Test**: This is one of the statistical tests for checking stationarity. Here the **null hypothesis is that the TS is non-stationary**. The test results comprise of a test statistic and some critical values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and affirm that the series is stationary. This is the most commonly used method to check stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries, col_with_data):\n",
    "    #Determine rolling statistics\n",
    "    movingAverage = timeseries.rolling(window=12).mean()\n",
    "    movingSTD = timeseries.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics\n",
    "    plt.plot(timeseries, color='blue', label='Original')\n",
    "    plt.plot(movingAverage, color='red', label='Rolling Mean')\n",
    "    plt.plot(movingSTD, color='black', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "\n",
    "    #Perform Dickey-Fuller test\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(\n",
    "        timeseries[col_with_data], # The data\n",
    "        autolag='AIC'            # Akaike Information Criterion, i.e., the method to estimate the optimal lag\n",
    "    )\n",
    "    # Extract and display test results in a user friendly manner\n",
    "    df_output = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    # for key, value in dftest[4].items():\n",
    "    #     df_output['Critical Value (%s)'%key] = value\n",
    "    print(df_output)\n",
    "\n",
    "test_stationarity(df, col_with_data='#Passengers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is greater than 0.05, we can say that the series is not stationary since we cannot reject the null hypothesis, i.e., by accepting the null hypothesis we are saying that the series is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the time series stationary\n",
    "There are some major reasons to analyse to check the non-stationarity of a TS:\n",
    "- **Trend** – varying mean over time. For e.g., in this case we saw that on average, the number of passengers was growing over time.\n",
    "- **Seasonality** – variations at specific time-frames\n",
    "- **Heteroskedasticity** - variance is increasing over time\n",
    "- **Autocorrelation** - correlation between the values of the same variables at different time points\n",
    "- **Cyclical** - regular up and down movements in the data that are not seasonal\n",
    "- **Outliers** - data points that are far away from other data points\n",
    "- **Level shifts** - sudden changes in the mean of the time series. E.g., the mean of the time series suddenly increases or decreases at a certain point in time because of some structural change in the process that’s generating the data.\n",
    "- **Missing values** - missing data points in the time series.\n",
    "\n",
    "We can see these easily in the plots above. Lets understand how to remove these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating Trend - remember that we can remove trend by differencing\n",
    "df_without_trend = df - df.rolling(window=12).mean()\n",
    "df_without_trend.dropna(inplace=True)\n",
    "\n",
    "test_stationarity(df_without_trend, col_with_data='#Passengers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can see that the mean is almost constant over time. Also, the test statistic is less than the critical value, so we can say that the series is stationary with 95% confidence.\n",
    "\n",
    "However, we still can see that the standard deviation is increasing slightly over time. We can remove this by taking the log of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = np.log(df)\n",
    "\n",
    "ax = df_log.plot()\n",
    "df_log.rolling(window=12).mean().plot(ax=ax)\n",
    "ax.legend(['log', 'log-rolling mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, time series with log scale as well as Rolling Mean (moving avg) both have the trend component. Thus subtracting one from the other should remove the trend component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollmean_log = df_log.rolling(window=12).mean()\n",
    "df_stat = df_log - rollmean_log\n",
    "df_stat.dropna(inplace=True)\n",
    "df_stat.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us perform the Dickey-Fuller test again on the resultant series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df_stat, col_with_data='#Passengers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we came to know that \"indeed subtracting two related series having similar trend components actually removed trend and made the dataset stationary\"\n",
    "\n",
    "Also, after concluding the results from ADFC test: p-value has reduced to 0.022, so we can say that the series is stationary with 95% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition\n",
    "\n",
    "Already seen in a previous section, decomposing a time series involves breaking down a time series data into its individual components in order to better understand the underlying patterns and trends. The three main components of a time series are:\n",
    "\n",
    "- __Trend__ - the long-term direction of the data over time. It represents the overall tendency of the series to increase, decrease, or remain relatively constant over time.\n",
    "\n",
    "- __Seasonality__ - the pattern of variations that occur within a specific time period, such as a day, week, month, or year. Seasonality is often due to predictable factors such as weather, holidays, or events.\n",
    "\n",
    "- __Random or irregular component__ - the unpredictable fluctuations or noise that cannot be explained by the trend or seasonality.\n",
    "\n",
    "By decomposing a time series into these three components, it becomes easier to identify and analyze the patterns and trends that may be occurring in the data. This can be particularly useful in forecasting future values and making informed decisions based on the insights gained from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Air passengers dataset\n",
    "Let us recall some examples. For the passengers dataset, for instance it is obvious the trend and the seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Load time series data\n",
    "df_passengers = pd.read_csv('./data/passengers_TS/passengers.csv', index_col='Month', parse_dates=True)\n",
    "df_passengers.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Perform  decomposition using the seasonal_decompose function from statsmodels\n",
    "decomposition = seasonal_decompose(df_passengers, model='multiplicative')\n",
    "\n",
    "# Plot the decomposition components\n",
    "fig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(10,6))\n",
    "\n",
    "axes[0].plot(df_passengers)\n",
    "axes[0].set_title('Original Data')\n",
    "\n",
    "axes[1].plot(decomposition.trend)\n",
    "axes[1].set_title('Trend')\n",
    "\n",
    "axes[2].plot(decomposition.seasonal)\n",
    "axes[2].set_title('Seasonality')\n",
    "\n",
    "axes[3].plot(decomposition.resid)\n",
    "axes[3].set_title('Residuals')\n",
    "\n",
    "# plot vertical dashed red lines each year\n",
    "for i in range(4):\n",
    "    for date in pd.date_range(\"1949\", \"1960\", 1960-1949+1):\n",
    "        axes[i].axvline(date, color=\"red\", linestyle=\":\")\n",
    "\n",
    "plt.tight_layout() # to avoid overlapping of the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sun spots dataset\n",
    "Let us see another example. The sun spots dataset is a time series data that represents the number of sun spots that were observed each year from 1749 to 2017. Let us a similar decomposition for this dataset. It was found that there a cycle of ~11 years in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series data\n",
    "df_sun_spots = pd.read_csv('./data/sunspots_TS/sunspots.zip', index_col='Date', parse_dates=True)\n",
    "df_sun_spots.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_sun_spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform  decomposition\n",
    "decomposition = seasonal_decompose(df_sun_spots, model='additive', period=11*12) # decompose with a 11*12 period\n",
    "\n",
    "# Plot the decomposition components\n",
    "fig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(20,10))\n",
    "\n",
    "axes[0].plot(df_sun_spots)\n",
    "axes[0].set_title('Original Data')\n",
    "\n",
    "axes[1].plot(decomposition.trend)\n",
    "axes[1].set_title('Trend')\n",
    "\n",
    "axes[2].plot(decomposition.seasonal)\n",
    "axes[2].set_title('Seasonality')\n",
    "\n",
    "axes[3].plot(decomposition.resid)\n",
    "axes[3].set_title('Residuals')\n",
    "\n",
    "# plot vertical dashed red lines\n",
    "for i in range(4):\n",
    "    for date in pd.date_range(\"1749-01-31\", \"2021-01-31\", int((2021-1749)/11)):\n",
    "        axes[i].axvline(date, color=\"red\", linestyle=\":\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us also performe the ADFC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df_sun_spots, col_with_data='Monthly Mean Total Sunspot Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value is approx. 0, the null hypothesis, which is that the time series is non-stationary, is rejected and we can say that the series is stationary with 95% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "**Autocorrelation** is a statistical concept that **measures the correlation between a time series and a lagged version of itself**. In other words, it measures how much a time series is correlated with its own past values at different time lags.\n",
    "\n",
    "To calculate the autocorrelation of a time series in Python, we can use the autocorrelation plot and acf functions from the statsmodels library.\n",
    "\n",
    "The plot generated by the plot_acf function shows the autocorrelation function (ACF) for a time series. The ACF measures the correlation between a time series and a lagged version of itself at different time lags.\n",
    "\n",
    "In the plot, the \"horizontal\" axis represents the lag at which the autocorrelation is calculated, while the \"vertical\" axis represents the value of the autocorrelation coefficient (between -1 and 1).\n",
    "\n",
    "The _shaded blue_ region in the plot represents the confidence intervals for the autocorrelation coefficient. Any autocorrelation coefficient that falls outside these intervals is considered statistically significant, indicating that there is likely some correlation between the time series and its lagged values at that particular lag.\n",
    "\n",
    "So, to interpret the plot, we can look for significant autocorrelation coefficients that fall outside the confidence intervals. These indicate that the time series is correlated with its lagged values at those lags. For example, if there is a significant positive autocorrelation at a lag of 1, this suggests that each value in the time series is positively correlated with the value that immediately precedes it.\n",
    "\n",
    "We can also look at the pattern of the autocorrelation coefficients to identify any underlying seasonality or trend in the data. For example, if there is a significant positive autocorrelation at a lag of 12, this suggests that there is a seasonal pattern with a period of 12 months in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Air passengers dataset\n",
    "\n",
    "We can observe that until lag of 12 (approx.), the autocorrelation coefficients are positive and significant. This suggests that each value in the time series is positively correlated with the value that immediately precedes it. Also, there is an increase in the autocorrelation coefficients at lag 12, which suggests that there is a seasonal pattern with a period of 12 months in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Calculate the autocorrelation coefficients\n",
    "autocorr = acf(df_passengers['#Passengers'], nlags=24)\n",
    "print('Autocorrelation coefficients:\\n', autocorr)\n",
    "print(autocorr)\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot_acf(df_passengers['#Passengers'], lags=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sun spots dataset\n",
    "We can observe that the autocorrelation changes monotony in cycles of ~11 years. This suggests that there is a seasonal pattern with a period of 11 years in the data. Also the autocorrelation sign changes around 5.5 years due to the incresing and decreasing cycles of the sun spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# use 3 cycles of 11 years\n",
    "nlags = 11*12*3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "\n",
    "# Calculate the autocorrelation coefficients\n",
    "autocorr = acf(df_sun_spots['Monthly Mean Total Sunspot Number'], nlags=nlags)\n",
    "print('Autocorrelation coefficients:\\n', autocorr)\n",
    "print(autocorr)\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot_acf(df_sun_spots['Monthly Mean Total Sunspot Number'], ax=ax, lags=nlags)\n",
    "\n",
    "# plot vertical dashed red lines at each 12 months\n",
    "for date in range(0, nlags, 12):\n",
    "        ax.axvline(date, color=\"red\", linestyle=\":\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral analysis\n",
    "\n",
    "Spectral analysis is a technique used in time series analysis to decompose a signal into its frequency components. The resulting frequency domain representation can help identify periodicities and patterns in the data that may not be apparent in the time domain. (See for example this video for an intuitive explanation of spectral analysis: https://www.youtube.com/watch?v=spUNpyF58BY\n",
    "\n",
    "In spectral analysis, we use the Fourier transform to convert a time series from the time domain to the frequency domain. The Fourier transform represents the time series as a sum of sine and cosine waves with different frequencies and amplitudes. This allows us to identify the specific frequencies that contribute to the overall behavior of the time series, and to analyze the data in terms of its periodicities.\n",
    "\n",
    "The resulting frequency domain representation is typically visualized by ploting the amplitude of each frequency component against the frequency.\n",
    "\n",
    "Spectral analysis can be useful for a variety of applications, including identifying seasonal patterns in economic or climate data, detecting periodicities in physiological signals, and analyzing fluctuations in financial markets. It can also be used to filter out noise or unwanted frequency components from a time series, allowing us to focus on the specific frequencies that are most relevant to the analysis at hand.\n",
    "\n",
    "Let us see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passengers' dataset\n",
    "First let us analyse the passengers data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import periodogram\n",
    "\n",
    "# Compute the periodogram - The periodogram function returns two arrays:\n",
    "# freq, which contains the frequencies, and\n",
    "# power, which contains the power at each frequency.\n",
    "freq, power = periodogram(df_passengers['#Passengers'])\n",
    "\n",
    "# Plot the power spectrum\n",
    "plt.plot(freq, power)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see 2 peaks in the power values with valueas above ~190,000 with a frequency arround 0.01 and 0.08. Let us try to find that value and from the frequency find the periodicity (we will use 50,000 which will also catch some of the other lower peaks - see the periodicity of ~12x$n$ months!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_freq = freq[np.where(power > 50000)]\n",
    "print('peak value', peak_freq)\n",
    "print('periodicity', 1/peak_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sun spots dataset\n",
    "And now a similar analysis for the sun spots data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import periodogram\n",
    "\n",
    "# Compute the periodogram\n",
    "freq, power = periodogram(df_sun_spots['Monthly Mean Total Sunspot Number'])\n",
    "\n",
    "# Plot the power spectrum\n",
    "plt.plot(freq, power)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the peak in the power of >3,000,000. Let us try to find that value and from the frequency above $10^6$ and from there find the (~11 year) periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_freq = freq[np.where((power>10**6) )]\n",
    "print('peak value:', peak_freq)\n",
    "print('seasonality:', 1/peak_freq)\n",
    "print('divided by 12 (months) to get the values in years:', 1/peak_freq/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and classification\n",
    "\n",
    "Clustering and classification techniques can be used to group similar time series together or classify them based on certain characteristics, which can provide insights into patterns or trends across multiple time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# compute the variation of the stocks over the quarters\n",
    "df_delta_quarter = compute_variation(df_stocks, 'QE')\n",
    "df_delta_quarter.dropna(inplace=True)\n",
    "# Clustering using K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10).fit(df_delta_quarter)\n",
    "clusters = kmeans.predict(df_delta_quarter)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data, maybe the cluster are small, medium and large variations in the stocks value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "for col in list_of_stocks:\n",
    "    ax.scatter(df_delta_quarter.index, df_delta_quarter[col], c=clusters, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis\n",
    "Correlation analysis can be used to examine the relationship between a time series and other variables, such as economic indicators or weather data.\n",
    "\n",
    "Correlation analysis is a widely used technique in time series analysis to study the relationship between two or more time series. Correlation analysis is used to determine whether and to what extent two time series are related to each other, and to identify patterns and trends in the data.\n",
    "\n",
    "There are two main types of correlation analysis that can be applied to time series data:\n",
    "\n",
    "- **Cross-correlation analysis**: Cross-correlation measures the similarity between two time series as a function of the time lag applied to one of them. Cross-correlation analysis can be used to identify the time lag that maximizes the correlation between the two time series, which can be useful for synchronizing data recorded at different times. Cross-correlation analysis can also be used to identify leading and lagging time series, which can help predict future behavior.\n",
    "\n",
    "- **Auto-correlation analysis**: as seen above auto-correlation measures the correlation between a time series and a lagged version of itself.\n",
    "\n",
    "Let us see an example supported on the stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cross-correlation between AAPL and META\n",
    "corr = df_stocks['Close_AAPL'].corr(df_stocks['Close_MSFT'])\n",
    "\n",
    "# Plot time series data\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.plot(df_stocks.index, df_stocks['Close_AAPL'], label='Close AAPL')\n",
    "plt.plot(df_stocks.index, df_stocks['Close_MSFT'], label='Close MSFT')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Correlation: ' + str(corr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the cross-correlation between the stocks data and the S&P 500 index, we can use the Pandas corrwith function, which computes the correlation between two DataFrames along their common index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-correlation between AAPL and META\n",
    "list_of_lags = range(-30, 120)\n",
    "cross_corr = [df_stocks['Close_AAPL'].corr(df_stocks['Close_MSFT'].shift(lag)) for lag in list_of_lags]\n",
    "print(cross_corr)\n",
    "plt.plot(list_of_lags, cross_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granger causality analysis (optional)\n",
    "\n",
    "Granger causality analysis is a statistical technique used to **investigate the causal relationship between two time series variables**. It is widely used in economics, finance, neuroscience, and other fields to study cause-and-effect relationships in time series data.\n",
    "\n",
    "The basic idea behind Granger causality analysis is to **test whether the past values of one time series variable can predict the future values of another time series variable**, over and above the effects of the other variable's own past values. If the past values of one variable can help predict the future values of the other variable, then it is said to have Granger-caused the other variable.\n",
    "\n",
    "Granger causality analysis typically involves the following steps:\n",
    "\n",
    "- Estimate the autoregressive models for each variable: This involves fitting a linear regression model to each time series variable, using its past values as predictors.\n",
    "\n",
    "- Test for Granger causality: This involves comparing the performance of the two models (with and without the lagged values of the first variable) using statistical tests such as the F-test or likelihood ratio test. If the inclusion of the lagged values of the first variable significantly improves the performance of the model, then it is said to Granger-cause the second variable.\n",
    "\n",
    "Granger causality analysis has some limitations and caveats that should be considered. For example, Granger causality analysis does not imply causality in a strict sense, and it cannot rule out the possibility of reverse causality or the presence of unobserved confounding variables. It also assumes that the time series data is stationary and linear, which may not be true for all applications. However, despite these limitations, Granger causality analysis can provide valuable insights into the causal relationships between time series variables and can help inform decision-making in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see an example with the stock market time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "maxlag=15\n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):\n",
    "    _df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in _df.columns:\n",
    "        for r in _df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            _df.loc[r, c] = min_p_value\n",
    "    _df.columns = [var + '_x' for var in variables]\n",
    "    _df.index = [var + '_y' for var in variables]\n",
    "    return _df\n",
    "\n",
    "grangers_causation_matrix(df_stocks[close_cols].dropna(), variables = df_stocks[close_cols].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row are the response (y) and the columns are the predictors (x). If a given p-value is less than the significance level (0.05) (for example, take the value 0.0 in (row 1, column 2)), we can reject the null hypothesis and conclude that Close_META_x Granger causes Close_GOOGL_x. Likewise, the 0.0004 in (row 2, column 1) refers to Close_GOOGLE_y Granger causes Close_META_x. The only case that misses the test is the Close_AAPL_y vs Close_TSLA_x, meaning that we cannot conclude that Close_AAPL_y Granger causes Close_TSLA_x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
