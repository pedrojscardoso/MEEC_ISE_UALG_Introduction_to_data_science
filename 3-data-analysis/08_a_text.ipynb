{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data: an introduction\n",
    "## What is text data?\n",
    "- Text data is a data structure that represents information in the form of text. It is a collection of words, sentences, and paragraphs that is readable and includes alphabets and numbers. Text data is everywhere, being one of the most common forms of data that is generated by humans in the form of blogs, tweets, comments, and so on.\n",
    "\n",
    "- But, text data is **unstructured data**, and it is not easy to extract information from it.  Text data is **also known as natural language data**.\n",
    "\n",
    "- NLP (Natural Language Processing) is a field of computer science that deals with the interaction between computers and humans using the natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of text data\n",
    "Text data is used in many applications, such as:\n",
    "- **SENTIMENT ANALYSIS**: Sentiment analysis is the process of analyzing the sentiment of a piece of text. It is used to determine whether the sentiment of a piece of text is positive, negative, or neutral. It is used in many applications, such as social media monitoring, brand monitoring, and customer service.\n",
    "  \n",
    "- **TEXT CLASSIFICATION**: Text classification is the process of classifying text into different categories. It is used in many applications, such as spam detection, sentiment analysis, and topic classification.\n",
    "  \n",
    "- **TEXT SUMMARIZATION**: Text summarization is the process of summarizing text into a shorter version. It is used in many applications, such as news summarization, document summarization, and email summarization.\n",
    "  \n",
    "- **MACHINE TRANSLATION**: Machine translation is the process of translating text from one language to another. It is used in many applications, such as language translation, document translation, and website translation.\n",
    "  \n",
    "- **QUESTION ANSWERING**: Question answering is the process of answering questions. It is used in many applications, such as question answering systems, question answering systems, and question answering systems.\n",
    "  \n",
    "- **INFORMATION RETRIEVAL**: Information retrieval is the process of retrieving information from a collection of documents.\n",
    "  \n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Basic definitions\n",
    "\n",
    "So, a text document is a collection of words, sentences, and paragraphs. All in all it not more than a string. Let us define some basic concepts:\n",
    "\n",
    "#### Corpus\n",
    " The **corpus** is (a large and) structured collection of texts or written materials that are used for linguistic analysis, research, or language modeling purposes. \n",
    " \n",
    "E.g., the collection of all the documents in a library, the collection of all the documents in a database, the collection of all the documents in a website, etc.\n",
    "\n",
    "#### Lexicon\n",
    "The **lexicon** of a language is the set of all words in that language. It is also called the **vocabulary** of the language.\n",
    "\n",
    "**words** are also called **terms** and, in some contexts, **tokens**.\n",
    "\n",
    "E.g., the lexicon of the English language is the set of all English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "#### $n$-grams\n",
    "\n",
    "$n$-grams are contiguous sequences of $n$ items from a given sample of text or speech. In the context of natural language processing, an $n$-gram typically refers to a sequence of $n$ words or characters.\n",
    "\n",
    "For example, let's consider the sentence: \"I love to code.\" Here are some examples of $n$-grams with different values of $n$:\n",
    "\n",
    "- Unigrams ($n = 1$): [\"I\", \"love\", \"to\", \"code\"]\n",
    "- Bigrams ($n = 2$): [\"I love\", \"love to\", \"to code\"]\n",
    "- Trigrams ($n = 3$): [\"I love to\", \"love to code\"]\n",
    "- 4-grams ($n = 4$): [\"I love to code\"]\n",
    "\n",
    "The longer the $n$-gram (the higher the value of $n$), the more context you have to work with. In general, a larger $n$-gram generally means more context, which means a better understanding of the structure and sentiment of a text. The optimal value of $n$ depends on the application and the dataset. For example, in spam detection, unigrams perform better than bigrams and trigrams, while for authorship attribution, character 4-grams work better than word 4-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary example\n",
    "Example, given the list of documents:\n",
    "- D1: \"I like to play football\"\n",
    "- D2: \"I hate football\"\n",
    "- D3: \"I like to play tennis\"\n",
    "\n",
    "As a result\n",
    "\n",
    "- The lexicon is: {I, like, to, play, football, hate, tennis}.\n",
    "- The corpus is: {D1, D2, D3}.\n",
    "- The 2-grams / bigrams are: {I like, like to, to play, play football, I hate, hate football, like to, to play, play tennis}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basic feature extraction techniques\n",
    "### Dataset\n",
    "\n",
    "In this notebook we'll use the **Large Movie Review Dataset v1.0**, retrieved from http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification.\n",
    "\n",
    "\n",
    "The core dataset contains 50,000 reviews split evenly into 25K train and 25K test sets. The overall distribution of labels is balanced (25K are positive and 25K are negative). It also includes an additional 50,000 unlabeled documents for unsupervised learning, which we will not use in this notebook.\n",
    "\n",
    "In the entire collection, \n",
    "- no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. \n",
    "\n",
    "- the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.  \n",
    "\n",
    "In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets. In the unsupervised set, reviews of any rating are included and there are an even number of reviews > 5 and <= 5.\n",
    "\n",
    "Please read the README (in the aclImdb.zip) for more details. We will use the file imdb_data.csv.zip which is the compiled version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/IMDB/imdb_data_train.zip')\n",
    "\n",
    "print('shape = ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a review is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> review:', df.loc[42, 'review'])\n",
    "print('--> classification:', df.loc[42, 'classification'])\n",
    "print('--> sentiment:', df.loc[42, 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this set of reviews builds out the corpus of text data. A number of operations can now be performed on this data. Let's start with some basic feature extraction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words\n",
    "\n",
    "Counting of words in a document is a basic feature extraction technique.\n",
    "\n",
    "We can use the `split()` function to count the number of words in a document. The split function splits a string into a list separated by a delimiter. The default delimiter is a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#words'] = df['review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_vs_feature__analysis(df, col):\n",
    "    \"\"\" function to analyze correlation between a column and classification \"\"\"\n",
    "    print(df[['classification', col]].corr())\n",
    "    df[['classification', col]].groupby(['classification']).mean().sort_values(by='classification', ascending=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there isn't a direct correlation between the number of words in a document and the sentiment of the document. \n",
    "\n",
    "Nevertheless, we can see that extreme classification tend to have less words than the others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_vs_feature__analysis(df, '#words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters\n",
    "Counting the number of characters in a document is also a basic feature extraction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#chars'] = df['review'].str.len()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your conclusion from the following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_vs_feature__analysis(df, '#chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length\n",
    "Average word length is a feature extraction technique that is used to find the average length of all the words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words_lens = [len(word) for word in sentence.split()]\n",
    "  return sum(words_lens)/len(words_lens)\n",
    "\n",
    "df['avg_word_len'] = df['review'].apply(lambda x: avg_word(x))\n",
    "\n",
    "df[['review', 'avg_word_len']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_vs_feature__analysis(df, 'avg_word_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Stopwords\n",
    "Stopwords are the words that are most commonly used in a language, such as \"the\", \"a\", \"an\", \"in\", and \"on\". **These words do not add any meaning to a sentence**. \n",
    "\n",
    "Stopwords are **removed to reduce the dimensionality of the data** and to **remove noise from the data** (e.g., see https://en.wikipedia.org/wiki/Stop_word).\n",
    "\n",
    "Obviously, stop words are language dependent.\n",
    "- In English, stopwords are, for example: \"the\", \"a\", \"an\", \"in\", \"on\", etc.\n",
    "- In Portuguese, stopwords are, for example: \"o\", \"a\", \"os\", \"as\", \"em\", \"sobre\", etc.\n",
    "- In Spanish, stopwords are, for example: \"el\", \"la\", \"los\", \"las\", \"en\", \"sobre\", etc.\n",
    "- In japanese, stopwords are, for example: \"の\", \"に\", \"は\", \"を\", \"た\", \"が\", etc. (!)\n",
    "\n",
    "We can use the `nltk` library to count the number of stopwords in a document. The `nltk` library is a collection of natural language processing libraries. It is used to perform various natural language processing tasks, such as tokenization, stemming, lemmatization, and so on. We'll some of these functional later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#stopwords'] = df['review'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_vs_feature__analysis(df, '#stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personalized stop words list\n",
    "\n",
    "**Using a preexisting collection of stop words may seem convenient, but it often proves inadequate for specific applications**. Take clinical texts, for instance, where words like \"mcg,\" \"dr.,\" and \"patient\" appear frequently in almost every document. In the context of clinical text mining and retrieval, these terms can be considered as potential stop words. Likewise, when dealing with tweets, terms like \"#,\" \"RT,\" and \"@username\" may qualify as potential stop words. Unfortunately, the standard list of language-specific stop words fails to encompass these domain-specific terms.\n",
    "\n",
    "To **set our own stop words**, as a thumb rule, we can use the following strategies:\n",
    "1. set the $n$-most frequent terms in the corpus as stop words\n",
    "2. set the $n$-least frequent terms in the corpus as stop words\n",
    "3. set the $n$-least IDF score terms as stop words (see below)\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of special characters\n",
    "Special characters include `!`, `@`, `#`, `$`, `%`, etc. We can use the `count()` function to count the number of special characters in a document. \n",
    "\n",
    "For instance, the number of exclamations can be used the level of excitement, surprise, anger etc. in a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#exclamations'] = df['review'].str.count('!')\n",
    "\n",
    "df[['review', '#exclamations']].sort_values(by='#exclamations', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[15598, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can the the number of exclamations points is higher in extreme reviews!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_vs_feature__analysis(df, '#exclamations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, in twitter special characters are used to tag topics, so counting special characters can be useful in topic detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#topics'] = df['review'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "df[['review', '#topics']].sort_values(by='#topics', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of numerics\n",
    "We can use the `isdigit()` function to count the number of numerics in a document.\n",
    "This can be useful in detecting spam, which often contains a lot of numerics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#numerics'] = df['review'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "df[['review', '#numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of uppercase words\n",
    "Uppercase words can be used to express anger or excitement. We can use the `isupper()` function to count the number of uppercase words in a document.\n",
    "In our case, maybe is not so useful, as we can see in uppercase words list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upper'] = df['review'].apply(lambda x: [x for x in x.split() if x.isupper()])\n",
    "df['#upper'] = df['upper'].apply(lambda x: len(x))\n",
    "\n",
    "df[['review', 'upper', '#upper']].sort_values(by='#upper', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(df.loc[9542, 'review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Pre-processing of text data\n",
    "So far, we have seem how to extract basic features from text data. Now, we will see how to pre-process text data before extracting features from it. **Pre-processing** refers to the transformations applied to our data before feeding it to some algorithm. In the context of text data, it is also known as **text cleaning and pre-processing**.\n",
    "\n",
    "So, text normalization, also known as text standardization, is a process that transforms text into a consistent or canonical form. Its purpose is to ensure uniformity and facilitate text processing and analysis. The normalization process is not a one-size-fits-all approach and can involve various techniques.\n",
    "\n",
    "For example, one common step in normalization is converting all text to lowercase. This straightforward and widely applicable method is effective for text pre-processing. Additionally, dealing with misspelled words, acronyms, short forms, and out-of-vocabulary terms is another approach. For instance, terms like \"super,\" \"superb,\" and \"superrrr\" can be normalized to \"super.\" By applying text normalization techniques, the noise and disruptions in the text data are handled, resulting in cleaner, more reliable data.\n",
    "\n",
    "Stemming and lemmatization are also employed as part of text normalization. Stemming reduces words to their base or root form, while lemmatization aims to bring words to their canonical or dictionary form. These techniques further contribute to word normalization in text processing.\n",
    "\n",
    "All in all, the texts are transformed from a sequencial list of words to a multidimensional vector of numbers, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $n$-grams\n",
    "Remember, an $n$-gram is a contiguous sequence of $n$ items from a given sample of text or speech.\n",
    "\n",
    "the nltk library provides a function to split text into $n$-grams. We can use the `ngrams()` function from the `nltk.util` module to generate $n$-grams from a sequence of tokens. The `ngrams()` function takes in two arguments: the sequence of tokens and the value of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the n-grams for the reviews\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# build the n-grams for the reviews\n",
    "df['review_ngrams'] = df['review'].apply(lambda x: list(ngrams(x.split(), 2)))\n",
    "\n",
    "df[['review', 'review_ngrams']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW)\n",
    "A **bag-of-words (BoW)** is a representation of text that describes the occurrence of words within a document. It keeps track of word counts and disregards the grammatical details and the word order.\n",
    "\n",
    "A possible assumption is that the higher the count of a word in a document, the more important it is and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `CountVectorizer()` function from the `sklearn.feature_extraction.text` module to perform CountVectorization (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). The `CountVectorizer()` function takes in a list of strings and converts it to a matrix of integers. Each row in the matrix represents a document and each column represents a word and the corresponding cell represents the count of that word in that document.\n",
    "\n",
    "Further, for this part, we'll restrict the corpus to the first 10 documents in the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the corpus is a list of strings (documents) to analyze\n",
    "corpus = df['review'].head(10)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize the CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observation can be made at once:\n",
    "- the matrix is sparse, i.e., most of the cells are zero\n",
    "- the matrix is not normalized, i.e., the number of words in each document is not taken into account\n",
    "- the matrix contains a lot of words that might not be useful for some analysis, e.g., \"a\", \"about\", \"above\", \"after\", etc. These words are called **stop words** and they need to be removed from the corpus before performing CountVectorization.\n",
    "- Depending on the context, some words have the same \"meaning\" but are written differently, e.g., \"actor\" and \"actress\" (should these words be merged?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer without stop words removal\n",
    "We can remove stopwords before performing CountVectorization or ask it to do it, by passing the list of stopwords to the `stop_words` parameter of the `CountVectorizer()` function or a string with the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix. Note the toarray() function is used to convert the sparse matrix to a dense matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer with stop words removal and $n$-grams\n",
    "We can extract $n$-grams by passing the value of $n$ to the `ngram_range` parameter of the `CountVectorizer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', \n",
    "                     ngram_range=(1, 2))\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaningfulness of the results\n",
    "Bag-of-words does not bring in any information on the meaning of the text. For example, if we consider these two sentences\n",
    "- “Text processing is easy but tedious.” and\n",
    "- “Text processing is tedious but easy.”\n",
    "\n",
    "a bag-of-words model would create the same vectors for both of them, even though they have different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_corpus = [\n",
    "    'Text processing is easy but tedious.',\n",
    "    'Text processing is tedious but easy.'\n",
    "]\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(small_corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer\n",
    "The `HashingVectorizer()` function from the `sklearn.feature_extraction.text` module is another technique that is used to extract features from text (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). It is also known as `Hashing Trick`.\n",
    "\n",
    "In this technique, we simply **apply a hash function to the terms to convert them into numeric values**. The assumption here is that the hash function will assign unique indexes to the terms and hence we will not need to store the vocabulary explicitly. This will help us save memory.\n",
    "\n",
    "Further, it turns a collection of text documents into a `scipy.sparse matrix` holding token occurrence counts (or binary occurrence information), possibly normalized. Used norms are 'l1' and 'l2', being computed as:\n",
    "- 'l1': $\\frac{x_{ij}}{\\sum x_{ij}}$\n",
    "- 'l2': $\\frac{x_{ij}}{\\sqrt{\\sum x_{ij}^2}}$ \n",
    "\n",
    "This strategy has several advantages:\n",
    "- it is very **low memory** scalable to large datasets as there is no need to store a vocabulary dictionary in memory.\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters.\n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there is **no state computed during fit**.\n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "- there is **no way to compute the inverse transform** (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "- there can be **collisions**: distinct tokens can be mapped to the same feature index. However, in practice this is **rarely** an issue if `n_features` is large enough (e.g. $2^{18}$ for text classification problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hv = HashingVectorizer(stop_words='english',\n",
    "                       n_features=500) # note: this is a very small vocabulary... in a real case, we would use a much larger vocabulary\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = hv.fit_transform(corpus)\n",
    "\n",
    "print(X.shape)\n",
    "# build a dataframe with the term-document matrix\n",
    "hv_df = pd.DataFrame(X.toarray())\n",
    "hv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, 'Shakespeare' is hashed to 401,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hv.transform(['Shakespeare']))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So,containing words hashed to 401 can be found in the following..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hv_df.loc[:, 401]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In row 0, we can find the \"Shakespeare\" word, as we can see below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "HTML(corpus.loc[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Howeever, for line 4, we can't find the \"Shakespeare\" word, as we can see below. Some other words are hashed to 401."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "HTML(corpus.loc[4])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing\n",
    "Another preprocessing technique is to lowercase the term-document matrix. This avoids having multiple copies of the same word just because it was capitalized differently.\n",
    "\n",
    "For instance, converting to lower case, is the default behavior of the `CountVectorizer()` function from the `sklearn.feature_extraction.text` module (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "But it can also be done manually, using the `lower()` function from the `str` module (https://docs.python.org/3/library/stdtypes.html#str.lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lower'] = df['review'].apply(lambda x: x.lower())\n",
    "\n",
    "df[['review', 'review_lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal\n",
    "Ponctuation might not be / is useful for (basic) text analysis. We can remove it using the `replace()` function from the `re` module (https://docs.python.org/3/library/re.html).\n",
    "\n",
    "The regular expression \"[^\\w\\s]\" matches everything that is not (^) a word character (alphanumeric character) or whitespace. then the `re.sub()` function replaces all the matches with empty string. (see https://regexr.com/ for more information about regular expressions and to test them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['review_no_punctuation'] = df['review_lower'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "df[['review', 'review_no_punctuation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal\n",
    "The remove of stopwords is a common preprocessing step in text analysis. In this example, the stopwords are removed using the `stopwords` corpus from the `nltk` module (https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_no_stopwords'] = df['review_no_punctuation']\\\n",
    "    .apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))\n",
    "\n",
    "df[['review', 'review_no_stopwords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent words removal\n",
    "**Sometimes**, we can also remove the most frequent words from the text data. These words **might** not be useful for text analysis as they are very common and do not provide any information about the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the words in the text\n",
    "words_frequence = df['review_no_stopwords']\\\n",
    "    .str\\\n",
    "    .split(expand=True)\\\n",
    "    .unstack()\\\n",
    "    .value_counts()\\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "words_frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 10 most frequent words\n",
    "words_to_remove = words_frequence[:10].index.tolist()\n",
    "\n",
    "print('words to remove:', words_to_remove)\n",
    "\n",
    "df['review_no_frequent_words'] = df['review_no_stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in words_to_remove]))\n",
    "\n",
    "df[['review_no_stopwords', 'review_no_frequent_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare words removal\n",
    "Similar, we can remove the rare words from the text data. These words might not be useful for text analysis as they are very rare and do not provide any information about the text. I.e., because they’re so rare, the association between them and other words is dominated by noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 10 rarest words\n",
    "words_to_remove = words_frequence[-10:].index.tolist()\n",
    "words_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_no_rare_words'] = df['review_no_stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (words_to_remove)]))\n",
    "\n",
    "df[['review_no_stopwords', 'review_no_rare_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction\n",
    "\n",
    "Text reviews (posts in generl) often contain spelling mistakes. We can use the `TextBlob()` function from the `textblob` module (https://textblob.readthedocs.io/en/dev/) to correct the spelling of the words (Spelling correction is based on Peter Norvig’s “How to Write a Spelling Corrector” as implemented in the pattern library. It is about 70% accurate...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# this is a time consuming process so we'll just do it for first 10 reviews\n",
    "df.loc[:10, 'review_corrected'] = df.loc[:10, 'review'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "df[['review', 'review_corrected']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of splitting a string into a list of pieces or tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. For instance, the sentence \"The cat is brown\" can be tokenized into the list of tokens ['The', 'cat', 'is', 'brown']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['review_tokenized'] = df['review_corrected'].apply(lambda x: word_tokenize(x))\n",
    "df[['review', 'review_tokenized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (from https://en.wikipedia.org/wiki/Stemming).  It chops off the prefixes and suffixes. \n",
    "\n",
    "For instance, the words 'fishing', 'fished', 'fishes' all stem from the word 'fish'. Stemming is useful in text analysis as it reduces the number of words to analyze.\n",
    "\n",
    "In this example, stemming can be done using the `SnowballStemmer()` class from the `nltk` module (https://www.nltk.org/). We should notice this is stemmer dependent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# create an instance of the SnowballStemmer class\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# define a function that applies the stemming to a list of words\n",
    "def stem_doc(doc):\n",
    "    return ' '.join([stemmer.stem(word) for word in doc.split()])\n",
    "\n",
    "sentence = 'actor actors actress actresses fish fishing fishes fished fisher am are is was were best well better good'\n",
    "\n",
    "for w1, w2 in zip(sentence.split(), stem_doc(sentence).split()):\n",
    "    print(f'{w1:10} -> {w2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stemming to the reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_stemmed'] = df['review'].apply(lambda x: stem_doc(x))\n",
    "\n",
    "df[['review', 'review_stemmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is the process of grouping together the inflected forms of a word, so they can be analysed as a single item, identified by the word's lemma, or dictionary form (from https://en.wikipedia.org/wiki/Lemmatisation).\n",
    "\n",
    "It is closely related to stemming. The main difference is that lemmatization **considers the context of the word while normalization** is performed.\n",
    "\n",
    "Lemmatization is useful in text analysis as it reduces the number of words to analyze.\n",
    "\n",
    "**Lemmatization vs. Stemming:** \n",
    "- Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. \n",
    "- Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. \n",
    "\n",
    "e.g., see (https://www.turing.com/kb/stemming-vs-lemmatization-in-python)[https://www.turing.com/kb/stemming-vs-lemmatization-in-python]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# nltk.download('wordnet') # download the WordNet corpus\n",
    "# nltk.download('omw-1.4') # download the Open Multilingual WordNet\n",
    "# nltk.download('treebank') # download the Treebank corpus\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define a function that applies the lemmatization to a list of words\n",
    "def lemmatize_doc(doc):\n",
    "    #PoS stands for \"part of speech\", the syntactic type of words, such as nouns, pronouns, adjectives, verbs, adverbs, and prepositions.\n",
    "    for pos in ['a', 's', 'r', 'n', 'v']: # a: adjective, s: adjective satellite, r: adverb, n: noun, v: verb\n",
    "        doc = ' '.join([lemmatizer.lemmatize(word, pos) for word in nltk.word_tokenize(doc)])\n",
    "    return doc\n",
    "\n",
    "\n",
    "for w1, w2 in zip(sentence.split(), lemmatize_doc(sentence).split()):\n",
    "    print(f'{w1:10} -> {w2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lemmatized'] = df['review'].apply(lambda x: lemmatize_doc(x))\n",
    "\n",
    "df[['review', 'review_lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatives can be found to the `WordNetLemmatizer()` class from the `nltk` module (https://www.nltk.org/). For instance,\n",
    "- the `spacy` module (https://spacy.io/)\n",
    "- the `textblob` module (https://textblob.readthedocs.io/en/dev/)\n",
    "- the `stanza` module (https://stanfordnlp.github.io/stanza/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance Text Processing\n",
    "\n",
    "More advanced text processing includes term frequency, inverse document frequency etc. These are covered in the following sections.\n",
    "\n",
    "So, first lets us re-read the data (considerer only the 100 first reviews) and do some basic text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_IMDB(dataset, nrows=None):\n",
    "    \"\"\" load the IMDB data and preprocess it:\n",
    "            - remove html tags\n",
    "            - remove ponctuation\n",
    "            - convert to lower case\n",
    "            - remove stop words\n",
    "            - remove numbers\n",
    "            - remove extra spaces\n",
    "            - replave words with their root form (stem)\n",
    "            - replace words with their lemma\n",
    "        :param dataset: 'train' or 'test'\n",
    "        :param nrows: number of rows to read\n",
    "        :return: df\n",
    "    \"\"\"\n",
    "\n",
    "    # read the data\n",
    "    df = pd.read_csv(f'./data/IMDB/imdb_data_{dataset}.zip', nrows=nrows)\n",
    "\n",
    "    # keep a copy of the original review\n",
    "    df['original_review'] = df['review']\n",
    "\n",
    "    # remove the html tags\n",
    "    df['review'] = df['review'].str.replace('<br />', ' ')\n",
    "\n",
    "    # remove the punctuation and '_' characters\n",
    "    df['review'] = df['review'].str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "    df['review'] = df['review'].str.replace('_', ' ', regex=False)\n",
    "\n",
    "    # convert to lower case\n",
    "    df['review'] = df['review'].str.lower()\n",
    "\n",
    "    # remove the stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    df['review'] = df['review'].apply(lambda doc: ' '.join([word for word in nltk.word_tokenize(doc) if word not in (stop_words)]))\n",
    "\n",
    "    # remove the numbers\n",
    "    df['review'] = df['review'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "    # remove the extra spaces\n",
    "    df['review'] = df['review'].str.replace(' +', ' ', regex=True)\n",
    "\n",
    "    # replace the words with their root form\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    df['review'] = df['review'].apply(lambda doc: ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(doc)]))\n",
    "\n",
    "    # replace the words with their lemma\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['review'] = df['review'].apply(lambda doc: ' '.join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(doc)]))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_preprocess_IMDB('train', nrows=100)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "The term frequency (TF) of a word is the frequency of the word (i.e. the number of times it appears) in a document. The term frequency is often divided by the sentence/document length (i.e. the total number of words in the sentence/document) as a way of normalization.\n",
    "\n",
    "So the term $t$ frequency in document $d$ is given by:\n",
    "$$\n",
    "\\mbox{term frequency}_{t,d}\n",
    "    = \\frac{\\mbox{number of times the t-word  appears in the d-document}}{\\mbox{total number of words in the d-document }}\n",
    "    = \\frac{n_{t,d}}{\\sum_w n_{w,d}}\n",
    "$$\n",
    "where $n_{t,d}$ is the number of times the $t$-word appears in the $d$-document.\n",
    "\n",
    "For instance, the term frequency of the word 'fish' in the sentence \"The fish is brown\" is 1/4 = 0.25.\n",
    "\n",
    "The term frequency ranges from 0 to 1. The higher the term frequency, the more \"important\" the word is to that document.\n",
    "\n",
    "Alternatives and tf-based solution include::\n",
    "- binary: 0, 1 (exists or not in the document)\n",
    "- raw count (term absolute frequency): $n_{i,d}$\n",
    "- log normalization (to reduce the impact of very frequent words): $\\log(1+n_{i,d})$ \n",
    "- double normalization(to avoid bias towards longer documents): $0.5 + 0.5\\frac{n_{i,d}}{\\max_{k \\in d} n_{k,d}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_frequency(doc):\n",
    "    \"\"\" Compute the term frequency of a word in a document\n",
    "    :param doc: the document\n",
    "    :return: the term frequency as a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the term absolute frequency by doc (i.e. the number of times the word appears in the document)\n",
    "    count_vect = CountVectorizer()\n",
    "    X = count_vect.fit_transform([doc])\n",
    "\n",
    "    # convert the term absolute frequency to a pandas dataframe\n",
    "    bow = pd.DataFrame(X.toarray(), columns=count_vect.get_feature_names_out())\n",
    "\n",
    "    # compute the term frequency by doc (i.e. the number of times the word appears in the document)\n",
    "    tf = bow.div(bow.sum(axis=1), axis=0)\n",
    "\n",
    "    return tf\n",
    "\n",
    "\n",
    "# compute the term frequency for each review\n",
    "df['review_tf'] = df['review'].apply(lambda doc: compute_term_frequency(doc).to_dict())\n",
    "\n",
    "df[['review', 'review_tf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) is a weight indicating **how commonly a word is used**.\n",
    "\n",
    "The IDF of a word is the measure of how significant that term is in the whole corpus (i.e. the list of all reviews).\n",
    "\n",
    "The inverse document frequency is computed by dividing the total number of documents in the corpus by the number of documents containing the word, and then taking the logarithm of that quotient, i.e.,\n",
    "$$ IDF_t\n",
    " = \\log\\left(\\frac{\\mbox{total number of documents in the corpus}}{\\mbox{number of documents containing the word}}\\right)\n",
    " = \\log\\left(\\frac{N}{|\\{d\\in D: t\\in d\\}|}\\right)\n",
    " $$\n",
    " where $N$ is the total number of documents in the corpus $D$,  and $|\\{d\\in D: t\\in d\\}|$ is the number of documents containing the $t$-word.\n",
    "\n",
    "The IDF value ranges from 0 to $\\infty$. **The closer the value is to 0, the more common the word is.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_idf(corpus):\n",
    "    \"\"\"\n",
    "    Compute the inverse document frequency for each word\n",
    "    :param corpus:\n",
    "    :return: ifd as a pandas series\n",
    "    \"\"\"\n",
    "    # fit and transform the vectorizer to the corpus\n",
    "    vectorizer = CountVectorizer(binary=True) # use binary=True to indicate the presence or absence of a word\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # convert the sparse matrix to a pandas dataframe\n",
    "    bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # compute the inverse document frequency for each word\n",
    "    return np.log(len(corpus) / bow.sum(axis=0))\n",
    "\n",
    "compute_idf(df['review']).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "The term frequency-inverse document frequency (TF-IDF) is the product of the term frequency and the inverse document frequency. The TF-IDF is used to measure how important a word is to a document in a collection of documents (i.e. a corpus). **The higher the TF-IDF, the more important the word is to that specific document**, i.e., relevant words in the document are expected to:\n",
    "- have a high term frequency (i.e. the word appears many times in the document)\n",
    "- have a high inverse-document frequency (i.e. the word appears in a small number of documents in the corpus)\n",
    "\n",
    "\n",
    "The **TF-IDF of a word in a document** is computed as follows:\n",
    "$$\n",
    "\\mbox{TF-IDF}_{t,d}\n",
    "    = \\mbox{term frequency}_{t,d} \\times \\mbox{inverse document frequency}_{t}\n",
    "    = \\frac{n_{t,d}}{\\sum_w n_{w,d}} \\times \\log\\left(\\frac{N}{|\\{d\\in D: t\\in d\\}|}\\right).\n",
    "$$\n",
    "\n",
    "For example, if the corpus has two sentence:\n",
    "- \"The fish is brown\"\n",
    "- \"The fish is green\"\n",
    "\n",
    "then the TF-IDF of the word 'fish' in the first sentence is: 0.25 * log(2/2) = 0.25 * 0 = 0 since\n",
    "- the term frequency of the word 'fish' in the first sentence is 0.25\n",
    "- the inverse document frequency of the word 'fish' is 0 since the word 'fish' appears in all the documents in the corpus. Since it appears in all the documents, we can't use it to differentiate between the documents...\n",
    "\n",
    "On the other hand, the TF-IDF of the word 'brown' in the first sentence is: 0.25 * log(2/1) = 0.25 * 0.693147 = 0.1733 since\n",
    "- the term frequency of the word 'brown' in the first sentence is 0.25\n",
    "- the inverse document frequency of the word 'brown' is 0.69 since the word 'brown' appears in only one document in the corpus.\n",
    "\n",
    "The TF-IDF ranges from 0 to $\\infty$. The higher the TF-IDF, the more important the word is to that document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the TF-IDF for some review\n",
    "def compute_tfidf(review, idf):\n",
    "    \"\"\" Compute the TF-IDF for a review\n",
    "    :param review: the review\n",
    "    :param idf: the inverse document frequency\n",
    "    :return: the TF-IDF as a pandas series\n",
    "    \"\"\"\n",
    "    # compute the term frequency\n",
    "    tf = compute_term_frequency(review)\n",
    "\n",
    "    # compute the TF-IDF\n",
    "    return tf.mul(idf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'The fish is brown',\n",
    "    'The fish was green',\n",
    "]\n",
    "\n",
    "# compute the TF-IDF for the first sentence\n",
    "compute_tfidf(corpus[0], compute_idf(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tfidf(corpus[1], compute_idf(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember\n",
    "\n",
    "    $$ \\mbox{TF-IDF}_{t,d}\n",
    "    = \\mbox{term frequency}_{t,d} \\times \\mbox{inverse document frequency}_{t}\n",
    "    = \\frac{n_{t,d}}{\\sum_w n_{w,d}} \\times \\log\\left(\\frac{N}{|\\{d\\in D: t\\in d\\}|}\\right).\n",
    "    $$\n",
    "\n",
    "So, a **high weight in TF-IDF** is reached by:\n",
    "- a high term frequency (in the given document) and\n",
    "- a low document frequency of the term in the whole collection of documents;\n",
    "\n",
    "The **weights hence tend to filter out common terms**.\n",
    "\n",
    "Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf–idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf–idf closer to 0.\n",
    "\n",
    "(From Wikipedia:)\n",
    "TF_IDF: \n",
    "- \"measures\" how important a word is to a document in a collection or corpus.\n",
    "- It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
    "- The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "- tf–idf has been one of the most popular term-weighting schemes. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing entities\n",
    "Recognizing entities is the process of identifying and classifying named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Let us use spaCy to recognize entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# create a doc object\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# print the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(pd.read_csv('./data/IMDB/imdb_data_train.zip')['review'].head(10)):\n",
    "    print()\n",
    "    print(doc)\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Word embeddings are a type of word representation that allows **words with similar meaning to have a similar representation**. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n",
    "\n",
    "For example, consider the following two sentences:\n",
    "- The cat sat on the mat.\n",
    "- The dog sat on the mat.\n",
    "\n",
    "In this example we have a vocabulary of 5 words. The sentences are 5 words long. We can represent each word using a one-hot encoding, i.e.:\n",
    "\n",
    "    cat = [1, 0, 0, 0, 0]\n",
    "    dog = [0, 1, 0, 0, 0]\n",
    "    mat = [0, 0, 1, 0, 0]\n",
    "    on = [0, 0, 0, 1, 0]\n",
    "    the = [0, 0, 0, 0, 1]\n",
    "\n",
    "We can then represent each sentence as a collection of vectors:\n",
    "\n",
    "        The cat sat on the mat = [[0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
    "        The dog sat on the mat = [[0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The folling code shows how to use the `Word2Vec` class from the `gensim` module (https://radimrehurek.com/gensim/models/word2vec.html) to create word embeddings. Uncomment the code to run it... it's quite computationally intensive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the Word2Vec model pre-trained on the Google News dataset\n",
    "# Note: This model is over 1.5GB, and loading it requires significant memory\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "# word_vector = model['computer']\n",
    "# print(word_vector)  # This prints the dense vector associated with 'computer'\n",
    "\n",
    "# Find the top 10 most similar words to 'computer'\n",
    "# similar_words = model.most_similar('computer', topn=10)\n",
    "# for word, similarity in similar_words:\n",
    "#    print(f\"{word}: {similarity}\")\n",
    "\n",
    "# Solve the analogy: man:king :: woman:?\n",
    "# result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "# print(result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud\n",
    "A word cloud is a visualization technique for text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually **single words, and the importance of each tag is shown with font size or color**. \n",
    "\n",
    "This format is useful for quickly perceiving the most prominent terms and for locating a term alphabetically to determine its relative prominence. When used as website navigation aids, the terms are hyperlinked to items associated with the tag.\n",
    "\n",
    "Let us use the wordcloud library to create a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a word cloud\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                      #stopwords=STOPWORDS,\n",
    "                      max_words=100,\n",
    "                      random_state=42\n",
    "                      ).generate(' '.join(df['review'].to_list()))\n",
    "\n",
    "# display the word cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "https://monkeylearn.com/sentiment-analysis/\n",
    "\n",
    "### What is it?\n",
    " - **Sentiment analysis** is a technique that uses natural language processing to **analyze the emotions in a piece of text**. It is also known as **opinion mining**, deriving the opinion or attitude of a speaker.\n",
    "\n",
    "- It can be used to analyze social media comments, product reviews, survey responses, and so much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of sentiment analysis\n",
    "There are two main types of sentiment analysis:\n",
    "- **Polarity detection**: Polarity detection is the most common type of sentiment analysis. It involves classifying a statement as either **positive, negative, or neutral**.\n",
    "- **Emotion detection**: Emotion detection is a more advanced type of sentiment analysis that detects emotions in a text. It involves detecting a whole range of emotions, such as, **joy, anger, disgust, sadness, fear, surprise, or anticipation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis using text classification\n",
    "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in natural language processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n",
    "\n",
    "The sentiment analysis process using text classification consists of the following steps:\n",
    "- **Data collection**: The first step is to collect the data. This data can be in the form of text, audio, or video. For example, if you want to analyze the sentiment of tweets, you’ll need to collect tweets that you want to analyze.\n",
    "- **Data labeling**: The next step is to label the data. This means that you need to manually assign a sentiment label to each piece of text. For example, if you want to analyze the sentiment of tweets, you’ll need to label each tweet as positive, negative, or neutral. Or with an emotion.\n",
    "- **Preprocessing the data**: The next step is to preprocess the data. This means that you need to clean the data and transform it into a format that can be used by a machine learning algorithm. For example, you can remove punctuation and convert all letters to lowercase.\n",
    "- **Training a text classification model**: The next step is to train a text classification model. This means that you need to feed the labeled data into a machine learning algorithm so that it can learn how to classify text. For example, you can train a text classification model to classify tweets as positive, negative, or neutral.\n",
    "- **Evaluating the model**: The final step is to evaluate the model. This means that you need to test the model on a set of data that it hasn’t seen before. For example, you can test the model on a set of tweets that it hasn’t seen before to see how well it can classify them.\n",
    "- **Deploying the model**: The final step is to deploy the model. This means that you need to make the model available for use. For example, you can deploy the model as a web service so that it can be used to analyze the sentiment of tweets.\n",
    "\n",
    "![text classification](./images/text_processing_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis using BoW\n",
    "\n",
    "Ley us start by remembering our dataset. We have the IMDB dataset of review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_IMDB('train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For which the distribution of sentiment is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='sentiment').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate the BoW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(lowercase=True, # it should already be in lower case...\n",
    "                                   stop_words='english', # stop words should already have been removed but ...\n",
    "                                   ngram_range = (1, 1))\n",
    "\n",
    "count_vectors_train = count_vectorizer.fit_transform(df['review'])\n",
    "count_vectors_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataframe with BoW and add the sentiment column (for an easier visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train = pd.DataFrame(count_vectors_train.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "bow_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we don't need to split the data into train and test, because that is already provided by the dataset.\n",
    "Let us just try is using a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# train the model (send bow_train or the count_vectors_train)\n",
    "rf.fit(bow_train, df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the test data and pass it through the BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_and_preprocess_IMDB('test')\n",
    "count_vectors_test = count_vectorizer.transform(df_test['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the BoW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = pd.DataFrame(count_vectors_test.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "bow_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And predict the sentiment and coresponding score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(bow_test, df_test['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(idx):\n",
    "    review = df_test.loc[idx, 'original_review']\n",
    "    target = df_test.loc[idx, 'sentiment']\n",
    "\n",
    "    classification = df_test.loc[idx, 'classification']\n",
    "    pred = rf.predict([bow_test.loc[idx]])\n",
    "    pred_proba = rf.predict_proba([bow_test.loc[idx]])\n",
    "\n",
    "    print(f'{\"OK\" if target == pred else \"!OK\"} / real sentiment: {target} (classfication: {classification}) / precicted sentiment: {pred} / pred_proba: {pred_proba}')\n",
    "    print(f'[{idx}]', review)\n",
    "\n",
    "import random\n",
    "\n",
    "for idx in random.sample(range(len(df_test)), 5):\n",
    "    info(idx)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'rf_model.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))\n",
    "\n",
    "# save the vectorizer to disk\n",
    "filename = 'count_vectorizer.sav'\n",
    "pickle.dump(count_vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Aggarwal, C. (2015). Data Mining: The Textbook. Springer.\n",
    "2. Navlani, A., Fandango, A., & Idris, I. (2021). Python Data Analysis: Perform Data Collection. In Data Processing, Wrangling, Visualization, and Model Building Using Python. Packt Publishing Ltd..\n",
    "2. Zong, C., Xia, R., & Zhang, J. (2021). Text data mining (Vol. 711, p. 712). Singapore: Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
