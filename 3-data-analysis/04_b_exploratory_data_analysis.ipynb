{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "<table align=\"left\" width=100%>\n",
    "    <td>\n",
    "        <div style=\"text-align: center;\">\n",
    "          <img src=\"./images/bar.png\" alt=\"entidades financiadoras\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <p style=\"text-align: center; font-size:24px;\"><b>Introduction to Data Science</b></p>\n",
    "        <p style=\"text-align: center; font-size:18px;\"><b>Master in Electrical and Computer Engineering</b></p>\n",
    "        <p style=\"text-align: center; font-size:14px;\"><b>Pedro Cardoso (pcardoso@ualg.pt)</b></p>\n",
    "    </td>\n",
    "</table>\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "__Short Lesson Title:__ Advanced EDA and Data Cleaning with Pandas\n",
    "\n",
    "*__Summary:__ This lesson builds upon introductory EDA concepts, diving deeper into data visualization and cleaning techniques using pandas, seaborn, and other libraries. It explores various plot types, including line, scatter, pie, histograms, bar, box, KDE, violin, heatmap, pair, joint, hexbin, and bubble plots, to visualize data distributions and relationships. The lesson then transitions to data cleaning, covering missing value treatment through dropping or filling (using mean, median, mode), and outlier detection and handling using scatter plots, boxplots, percentiles, z-scores, isolation forests, and autoencoders. Students will learn to apply these techniques to real-world datasets, gaining practical skills in preparing data for further analysis or modeling.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exploratory Data Analysis with Pandas (part 2)\n",
    "\n",
    "Let us continue with the exploratory data analysis with Pandas. We will continue with the titanic dataset. So, load the titanic dataset and remember its structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_bokeh\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "pandas_bokeh.output_notebook()\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data visualization\n",
    "\n",
    "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\n",
    "\n",
    "Let us see how we can use data visualization to explore the titanic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Line plot\n",
    "\n",
    "Line plots are useful to visualize the trend of a numerical column, usually, over time. We can use the `plot()` with the `kind='line'` argument from Pandas to visualize the trend of a numerical column.\n",
    "\n",
    "Since the Titanic dataset does not have a time column, we will use the Covid dataset to visualize the trend of the number of confirmed cases over time. The data is available at the \"COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University\" (https://github.com/CSSEGISandData/COVID-19).\n",
    "\n",
    "To load the data, we will use the `read_csv()` function from Pandas and the https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# url with the data\n",
    "url_confirmed = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "df_covid_all = pd.read_csv(url_confirmed, header=0)\n",
    "df_covid_all.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Since there are almost 300 rows, let us select only a few countries ('Portugal', 'Spain', 'Italy', 'France', 'Germany', 'United Kingdom'), group the data for countries with more than one region, convert the date column to datetime, and set it as index."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# select some countries\n",
    "mask = df_covid_all['Country/Region'] \\\n",
    "    .isin(['Portugal', 'Spain', 'Italy', 'France', 'Germany', 'United Kingdom'])\n",
    "\n",
    "# group by country and sum the values, drop the lat and long columns, and transpose the dataframe\n",
    "df_covid = df_covid_all[mask] \\\n",
    "    .drop(['Lat', 'Long', 'Province/State'], axis=1) \\\n",
    "    .groupby('Country/Region').sum() \\\n",
    "    .T\n",
    "\n",
    "# set the index to datetime\n",
    "df_covid.set_index(pd.to_datetime(df_covid.index, format='%m/%d/%y'), inplace=True)\n",
    "\n",
    "df_covid.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A few countries have been removed, and the data is now in a more convenient format. We can now visualize the trend of the number of confirmed cases over time for the selected countries."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_covid.plot(kind='line', figsize=(15, 5))\n",
    "plt.title('COVID-19 confirmed cases')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of confirmed cases')\n",
    "plt.legend(title='Country/Region')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_covid.plot(kind='area', figsize=(15, 5), alpha=0.5)\n",
    "plt.title('COVID-19 confirmed cases')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of confirmed cases')\n",
    "plt.legend(title='Country/Region')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the `plot_bokeh()` method from Pandas, we can create interactive plots. The `plot_bokeh()` method is similar to the `plot()` method, but it uses the Bokeh library to create interactive plots. The `plot_bokeh()` method is useful to create interactive plots that can be used in Jupyter notebooks or in web applications."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_covid.plot_bokeh(figsize=(800, 400),\n",
    "                    title='COVID-19')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_covid.plot_bokeh(figsize=(800, 400),\n",
    "                    kind='area',\n",
    "                    title='COVID-19')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scatter plot\n",
    "\n",
    "A scatter plot is useful to visualize the relationship between two numerical columns. We can use the `plot()` with the `kind='scatter'` argument from Pandas to visualize the relationship between two numerical columns.\n",
    "\n",
    "Let us return to the Titanic dataset and visualize the relationship between the age and the number of siblings and spouses aboard the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_titanic = pd.read_excel('./data/titanic/Titanic.xls')\n",
    "df_titanic.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_titanic.plot(kind='scatter', x='age', y='sibsp')\n",
    "plt.title('Age vs Number of siblings/spouses aboard')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of siblings/spouses aboard')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the covid dataset, we can visualize the relationship between the number of confirmed cases in Portugal and Spain. We can use the `plot()` with the `kind='scatter'` argument from Pandas to visualize the relationship between two numerical columns."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_covid.plot(kind=\"scatter\", x=\"Portugal\", y=\"Spain\")\n",
    "plt.title('Portugal vs Spain')\n",
    "plt.xlabel('Portugal')\n",
    "plt.ylabel('Spain')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pie chart and donut chart\n",
    "\n",
    "Pie charts are useful to visualize the distribution of a categorical column. We can use the `plot()` with the `kind='pie'` argument from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# pivot table counting survivers per sex and class\n",
    "df_survivers_per_sex_vs_class = df_titanic.pivot_table(index='sex',\n",
    "                                                       columns='pclass',\n",
    "                                                       values='survived',\n",
    "                                                       aggfunc='sum')\n",
    "\n",
    "print(df_survivers_per_sex_vs_class)\n",
    "\n",
    "df_survivers_per_sex_vs_class.plot(kind='pie',\n",
    "                                   subplots=True,\n",
    "                                   figsize=(10, 10),\n",
    "                                   )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Histograms\n",
    "We already have seen how to visualize the distribution of a numerical column using the `hist()` method from Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ax = df_titanic['age'].plot(kind='hist',\n",
    "                            alpha=0.1,\n",
    "                            bins=20,\n",
    "                            figsize=(10, 4),\n",
    "                            color='green')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Age distribution')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Seaborn also provides the `histplot()` method to visualize the distribution of a numerical column, but with more options.\n",
    "(https://seaborn.pydata.org/generated/seaborn.histplot.html).\n",
    "- The `element` parameter can be used to specify the type of histogram to plot.\n",
    "- The `kde` parameter can be used to specify whether to plot the kernel density estimate (KDE) or not (). The KDE is a non-parametric (i.e., does not assume a particular distribution) way to estimate the probability density function of a random variable. The KDE is a smooth curve that represents the distribution of the data.\n",
    "- The `bins` parameter can be used to specify the number of bins to use for the histogram.\n",
    "- The `cumulative` parameter can be used to specify whether to plot the cumulative histogram or not."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.histplot(data=df_titanic[['age']].dropna(),\n",
    "             x='age',\n",
    "             cumulative=False,  # {True, False}\n",
    "             bins=20,\n",
    "             kde=True,\n",
    "             element='poly'  # {‘bars’, ‘step’, ‘poly’}\n",
    "             )\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bar plot\n",
    "\n",
    "Bar plots are useful to visualize the distribution of a categorical column. We can use the `plot()` with the `kind='bar'` argument from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_survivers_per_sex_vs_class.plot(kind='bar', figsize=(10, 4))\n",
    "plt.title(\"Survived per class and sex\")\n",
    "plt.xlabel(\"Sex\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `plot()` method with the `kind='barh'` argument can be used to plot a horizontal bar plot."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_survivers_per_sex_vs_class.plot(kind='barh', figsize=(10, 4))\n",
    "plt.title(\"Survived per class and sex\")\n",
    "plt.xlabel(\"Sex\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Box plot\n",
    "\n",
    "Box plots are useful to visualize the distribution of a numerical column. We can use the `boxplot()` method from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_titanic[['age', 'fare']].boxplot(figsize=(10, 4))\n",
    "plt.title('Age and Fare distribution')\n",
    "plt.xlabel('Age and Fare')\n",
    "plt.ylabel('Count / Fare') # doesn't make much sense as a label because the two columns are not in the same scale\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The seaborn library also provides the `boxplot()` method to visualize the distribution of a numerical column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.boxplot(data=df_titanic['age'])\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the `x`, `y` in seaborn's `boxplot()` method, we can visualize the distribution of a numerical column by a categorical column. For example, we can visualize the distribution of the age by the passenger class."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ax = sns.boxplot(data=df_titanic,\n",
    "                 x='pclass',\n",
    "                 y='age',\n",
    "                 hue='sex')\n",
    "\n",
    "ax.set_title('Age distribution by class')\n",
    "ax.set_xlabel('Passenger Class')\n",
    "ax.set_ylabel('Age')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Kernel density estimation (KDE)\n",
    "Kernel density estimation is a non-parametric approach to estimating the distribution of data. Instead of assuming a particular distribution, we use a continuous representation of the data. For example, let's say we have a set of data measurements but we don't know their underlying distribution. We can use a Gaussian kernel to estimate the density around the data.\n",
    "\n",
    "We can use the `kdeplot()` method from Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.kdeplot(data=df_titanic, x='age', hue='pclass')\n",
    "plt.title('Age distribution by class')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Violin plot\n",
    "\n",
    "Violin plots are useful to visualize the distribution of a numerical column. Violin plots are similar to box plots, but they also show the probability density of the data at different values.\n",
    "\n",
    "We can use the `violinplot()` method from Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.violinplot(data=df_titanic['age'])\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By setting the `x` and `y` parameters, we can visualize the distribution of a numerical column by a categorical column. For example, we can visualize the distribution of the age by the passenger class. The `hue` parameter can be used to specify the categorical column to use for the color of the violin plot."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ax = sns.violinplot(data=df_titanic,\n",
    "                    x='pclass',\n",
    "                    y='age',\n",
    "                    hue='sex')\n",
    "\n",
    "ax.set_title('Age distribution by class')\n",
    "ax.set_xlabel('Passenger Class')\n",
    "ax.set_ylabel('Age')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Heatmap\n",
    "\n",
    "Heatmaps are useful to visualize the correlation between numerical columns. We can use the `heatmap()` method from Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# numerical columns\n",
    "numerical_columns = df_titanic.select_dtypes(include=np.number).columns\n",
    "\n",
    "sns.heatmap(df_titanic[numerical_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pairplot\n",
    "\n",
    "Pairplots are useful to visualize the relationship between multiple numerical columns. We can use the `pairplot()` method from Seaborn. Several options are available to customize the pairplot, try some of them changing the `kind` and `diag_kind` arguments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.pairplot(df_titanic,\n",
    "             kind='reg',  # {‘scatter’, ‘kde’, ‘hist’, ‘reg’}\n",
    "             diag_kind='kde',  # ‘hist’, ‘kde’,\n",
    "             hue='survived')\n",
    "\n",
    "plt.title('Pairplot of the numerical columns')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Jointplot\n",
    "Jointplots are useful to visualize the relationship between two numerical columns. We can use the `jointplot()` method from Seaborn which shows a scatter plot and the distribution of each variable. See the documentation for more options (https://seaborn.pydata.org/generated/seaborn.jointplot.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ax = sns.jointplot(data=df_titanic,\n",
    "              x='age', y='fare',\n",
    "              hue='survived',\n",
    "              kind='scatter'  #{ “scatter” | “kde” | “hist” | “hex” | “reg” | “resid” }\n",
    "              )\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hexbin plot\n",
    "\n",
    "Hexbin plots are useful to visualize the relationship between two numerical columns. We can use the `hexbin()` method from seaborn. The hexbin plot is similar to the scatter plot, but it uses hexagons instead of points to represent the data. The hexbin plot is useful to visualize the density of the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.jointplot(data=df_titanic,\n",
    "              x='age',\n",
    "              y='fare',\n",
    "              kind='hex'  # { \"scatter\" | \"kde\" | \"hist\" | \"hex\" | \"reg\" | \"resid\"}\n",
    "              )\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bubble plot\n",
    "\n",
    "Bubble plots are useful to visualize the relationship between two numerical columns and a __third numerical column__. We can use the `scatter()` method from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_titanic.plot.scatter(x='age',\n",
    "                        y='fare',\n",
    "                        s='pclass',     # sets the size of the bubbles which are small! 1, 2, 3!\n",
    "                        figsize=(10, 8)\n",
    "                        )\n",
    "plt.title('Age vs Fare vs Pclass')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, we can set the size of the bubbles to be proportional to the `pclass` column. The `s` parameter can be used to set the size of the bubbles. The size of the bubbles is set to be proportional to the `pclass` column, so we need to set the size of the bubbles to be a function of the `pclass` column.\n",
    "\n",
    "We can use the `c` parameter to set the color of the bubbles. The color of the bubbles is based on the `survived` column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_titanic.plot.scatter(x='age',\n",
    "                             y='fare',\n",
    "                             s=4 ** (4 - df_titanic['pclass']),\n",
    "                             c=df_titanic['survived'].apply(lambda x: 'red' if x == 1 else 'blue'),\n",
    "                             figsize=(10, 8),\n",
    "                             alpha=0.5\n",
    "                             )\n",
    "\n",
    "# the title is set to be bold, of size 16, blue color, and in times font\n",
    "plt.title('Age vs Fare', fontsize=16, fontweight='bold', color='red', loc='center', pad=20, fontname='courier new')\n",
    "plt.xlabel('Age', fontname='courier new')\n",
    "plt.ylabel('Fare', fontname='courier new')\n",
    "plt.grid(color='gray', linestyle='-.', linewidth=0.5)\n",
    "plt.xticks(fontsize=10, fontname='courier new')\n",
    "plt.yticks(fontsize=10, fontname='courier new')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that plot can be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "df_survived = df_titanic[df_titanic['survived'] == 1]\n",
    "df_not_survived = df_titanic[df_titanic['survived'] == 0]\n",
    "\n",
    "# Plot for non-survivors (survived == 0)\n",
    "df_survived.plot.scatter(\n",
    "    x='age',\n",
    "    y='fare',\n",
    "    s=4 ** (4 - df_survived['pclass']),\n",
    "    color='blue',\n",
    "    alpha=0.5,\n",
    "    label='Did Not Survive',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Plot for survivors (survived == 1)\n",
    "df_not_survived.plot.scatter(\n",
    "    x='age',\n",
    "    y='fare',\n",
    "    s=4 ** (4 - df_not_survived['pclass']),\n",
    "    color='red',\n",
    "    alpha=0.5,\n",
    "    label='Survived',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Age vs Fare by Survival Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "\n",
    "# Legend, grid, and ticks\n",
    "plt.legend(title='Survival Status')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "Real-world datasets are typically characterized by their messy and noisy nature, which often results in numerous faulty or missing values. Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset.\n",
    "\n",
    "Data cleaning is a fundamental step in the data analysis process. The data cleaning process involves many steps, such as:\n",
    "- **Missing values treatment** - remove or impute the missing values with values that are more representative of the data. E.g., the mean, the median, the mode, etc.\n",
    "\n",
    "- **Outliers treatment** - remove or transform the outliers.\n",
    "\n",
    "- **Feature engineering** - create new features from the existing features. E.g., create a new feature that is the sum or product of two other features.\n",
    "\n",
    "- **Feature selection** - remove features that are not useful for the analysis. E.g., remove features that have a low correlation with the target variable if the goal is to predict the target variable.\n",
    "\n",
    "- **Feature scaling** - scale the features to have the same range. E.g., scale the features to have values between 0 and 1. This is useful for some machine learning algorithms.\n",
    "\n",
    "- **Feature transformation** - transform the features to have a more normal distribution. E.g., transform the features to have a normal distribution. This is useful for some machine learning algorithms.\n",
    "\n",
    "- **Feature discretization** - discretize the features to have a more discrete distribution. E.g., transform real values to integer values. This is useful for some machine learning algorithms.\n",
    "\n",
    "- **Feature extraction** - extract features from the data. This is useful for some machine learning algorithms. E.g., extract features from text data, such as, the number of words or the number of characters.\n",
    "\n",
    "- **Feature encoding** - encode the features to have a more machine learning friendly format. E.g., encode the categorical features to have integer values.\n",
    "\n",
    "- **Feature reduction** - reduce the number of features. E.g., reduce the number of features using PCA. This is useful for some machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Missing values treatment\n",
    "\n",
    "Values that are not present in the data are referred to as missing values. They may be absent due to a variety of reasons, including human error, privacy concerns, or the failure of the survey respondent to complete the value. Missing values pose a significant challenge in data science and are typically addressed during data preprocessing. E.g., missing values can negatively impact the performance of machine learning models.\n",
    "\n",
    "There are various methods to handle missing values, including **dropping** the records that contain missing values, manually **filling** in the missing values, using **measures of central tendency** such as mean, median, or mode to fill in the missing values, or employing **machine learning models** such as regression, decision trees, or KNNs to predict and fill in the missing values with the most probable value.\n",
    "\n",
    "Let's see how many missing values we have in each column. First, we can use the `isnull()` method from Pandas to get a **boolean dataframe** with True for the missing values and False for the non-missing values. Then, we can use the sum() method to get the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df = df_titanic.copy()\n",
    "\n",
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Similarly, we can use the `isna()` method from Pandas to get a boolean dataframe with True for the missing values and False for the non-missing values. Then, we can use the sum() method to get the number of missing values in each column.\n",
    "\n",
    "The `isna()` method is an alias for the `isnull()` method, so both methods return the same result."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Drop missing values\n",
    "\n",
    "If we have a lot of data, we can drop the rows or columns that have missing values. We can use the `dropna()` method from Pandas to drop the rows or columns that have missing values. By default, the `dropna()` method drops the rows that have missing values. This will not work for the titanic dataset because every row has at least one missing value, as we can see from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": "Adding `axis=1` will drop the columns that have missing values. In the case it will drop age, fare, cabin etc."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": "df.dropna(axis=1)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fill missing values\n",
    "\n",
    "If we have a small dataset, we can fill the missing values with values that are more representative of the data. \n",
    "\n",
    "We can use the `fillna()` method from Pandas to fill the missing values with values that are more representative of the data. E.g., the mean, the median, the mode, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the age and fare columns, __numerical columns__, were 263 values are unknown, we can fill the missing values with the mean age and median fare. If the data was normally distributed, we could have used the mean or median, but when the data is not normally distributed, we should use the median. Have a look at the histograms and skew values of the age and fare columns to see that age is \"more normally distributed\" than the fare.\n",
    "\n",
    "So impirically, we can use the mean for the age and the median for the fare."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "\n",
    "df['fare'] = df['fare'].fillna(df['fare'].median())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The embarked column is a __categorical column__, so we can fill the missing values with the most frequent value which is given by the mode.\n",
    "\n",
    "For the embarked column, we can fill the missing values with the most frequent value which is given by the **mode**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['embarked'].mode()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the boat we can see that there are 823 missing values. This is because the boat column is only filled in for passengers who survived. We can fill the missing values with -1 as a value that represents that the passenger did not survive. We can also remove the boat column because if we think it is not useful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df['boat'] = df['boat'].fillna(-1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Shapiro-Wilk test for normality (Optional)\n",
    "\n",
    "**To test the normality of the distribution we can use the Shapiro-Wilk test**. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. If the p-value is less than an $\\alpha$ (e.g., a tipical value is $\\alpha = 0.05$), we can reject the null hypothesis and conclude that the data is not normally distributed. Otherwise, we **can not** reject the null hypothesis and conclude that the data is normally distributed. Usually, the significance level is set to 0.05 (5%), implying that it is acceptable to have a 5% probability of incorrectly rejecting the true null hypothesis. Further,  $p$ is basically the probability of finding our data if the null hypothesis is true.\n",
    "\n",
    "For our numerical variables, we can compute the p-values using the `shapiro()` function from the `scipy.stats` module. The `shapiro()` function returns the test statistic and the p-value. We can use the `select_dtypes()` method from Pandas to select only the numerical columns. Then, we can use a for loop to iterate over the numerical columns and print the p-value for each column. Further note the difference between original data and the data after filling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "def test_shapiro(df):\n",
    "    # for the numerical columns\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        p = shapiro(df[col].dropna())[1]\n",
    "        if p < 0.05:\n",
    "            print(\n",
    "                f\"p-value for {col}: {p} (reject the null hypothesis, i.e., the sample does NOT look like a normal distribution)\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"p-value for {col}: {p}  (null hypothesis is not rejectable, i.e., sample looks like a normal distribution)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_shapiro(df_titanic)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "See the Shapiro-Wilk test applied to some distributions in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 5000\n",
    "df_NUBP = pd.DataFrame({\n",
    "    'N': np.random.normal(0, 1, k),\n",
    "    'U': np.random.uniform(0, 1, k),\n",
    "    'B': np.random.binomial(100, 0.5, k),\n",
    "    'P': np.random.poisson(10, k),\n",
    "})\n",
    "\n",
    "# plot the histograms\n",
    "df_NUBP.hist(bins=20)\n",
    "\n",
    "# test the normality\n",
    "test_shapiro(df_NUBP)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Outliers treatment\n",
    "\n",
    "**Outliers are data points that are significantly different from the rest of the data**. Outliers can be caused by measurement errors or by human errors. Outliers can have a significant impact on the analysis, so it is important to treat them. The outliers treatment can be done in two ways:\n",
    "- **Remove the outliers**.\n",
    "- **Transform the outliers**.\n",
    "\n",
    "To identify outliers we can use, e.g., the following methods:\n",
    "\n",
    "- **Scatter plot** - the scatter plot is a graphical representation of the relationship between two variables and it is used to identify outliers. The outliers are the data points that are far away from the rest of the data.\n",
    "\n",
    "- **Interquartile range / Boxplot** - the boxplot is a graphical representation of the distribution of the data. The boxplot shows the median, the first quartile, the third quartile, the minimum, the maximum, and the outliers. The outliers are the data points that are outside the range $[Q_1 - 1.5 \\cdot IQR, Q_3 + 1.5 \\cdot IQR]$, where $Q_1$ is the first quartile, $Q_3$ is the third quartile, and $IQR = Q_3 - Q_1$ is the interquartile range.\n",
    "\n",
    "- **Percentile** - the outliers are the data points that are outside the range $[P_{\\alpha/2}, P_{100-\\alpha/2}]$, where $P_{x}$ is the $x$-th percentile.\n",
    "\n",
    "- **Z-score** - the z-score is a measure of how many standard deviations away from the mean a data point is. The outliers are the data points that have a z-score greater than 3 or less than -3.\n",
    "\n",
    "- **Isolation forest** - the isolation forest is an unsupervised machine learning algorithm that is used to identify outliers. The outliers are the data points that are isolated from the rest of the data.\n",
    "\n",
    "- **Auto encoder** - the auto encoder is an unsupervised machine learning algorithm that is used to identify outliers. The outliers are the data points that are not reconstructed well by the auto encoder.\n",
    "\n",
    "Let us see some examples of outliers treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Scatter plot\n",
    "\n",
    "If we plot the scatter plot of the `sibsp` (Number of siblings/spouses aboard) and the `parch` (Number of Parents/Children Aboard), we can see that there are a possible outlier with a `parch` equal to 9 and `sibsp` equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# copy the original dataframe\n",
    "df = df_titanic.copy()\n",
    "\n",
    "_ = df.plot_bokeh.scatter(x='sibsp', y='parch')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see thar there is a single family that fits the description of the outlier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df[(df.parch == 9) & (df.sibsp == 1)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By the ticket number, we can see the family's composition."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df[df.ticket == 'CA. 2343']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  Interquartile range / Boxplot\n",
    "\n",
    "The boxplot is a graphical representation of the distribution of the data. The boxplot shows the median, the first quartile, the third quartile, the minimum, the maximum, and the outliers. The outliers are the data points that are outside the range \n",
    "$$[Q_1 - 1.5 \\cdot IQR, Q_3 + 1.5 \\cdot IQR],$$ \n",
    "where $Q_1$ is the first quartile, $Q_3$ is the third quartile, and \n",
    "$$IQR = Q_3 - Q_1$$ \n",
    "is the interquartile range.\n",
    "\n",
    "Using the boxplot, we can see that there are outliers in the `age`, `sibsp`, `parch`, and `fare`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_ = df.boxplot()\n",
    "plt.title('Boxplot of the numerical columns')\n",
    "plt.xlabel('Numerical columns')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For example, we can see those outliers in the `fare` feature."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def get_outliers_mask_IQR(df, feature):\n",
    "    # compute the first, third quartiles and the interquartile range\n",
    "    q1, q3= df[feature].quantile(0.25), df[feature].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # compute the lower and upper bounds\n",
    "    lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "\n",
    "    # compute the mask for the outliers\n",
    "    mask_outliers = (df[feature] < lower_bound) | (df[feature] > upper_bound)\n",
    "    return mask_outliers\n",
    "\n",
    "\n",
    "mask_outliers = get_outliers_mask_IQR(df, 'fare')\n",
    "outliers = df[mask_outliers]\n",
    "outliers.sort_values(by='fare', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Removing the outliers, we can see that the boxplot is more compact, although outliers are still visible.\n",
    "Note the __`~` operator__ that is used to invert the mask. The `~` operator is a bitwise NOT operator that inverts the boolean values in the mask. So, `~mask_outliers` will be True for the non-outliers and False for the outliers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df.loc[~mask_outliers, ['fare']].boxplot()\n",
    "plt.title('Boxplot of the fare without outliers')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The process of removing the outliers can be done iteratively, i.e., removing the outliers and then removing the outliers of the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "mask_outliers_it_2 = get_outliers_mask_IQR(df[~mask_outliers], 'fare')\n",
    "\n",
    "while (mask_outliers_it_2.sum() > 0):\n",
    "    # update the outliers mask\n",
    "    mask_outliers = mask_outliers | mask_outliers_it_2\n",
    "\n",
    "    # plot the boxplot of the remaining data\n",
    "    df.loc[~mask_outliers, ['fare']].boxplot()\n",
    "    plt.title(f'Boxplot of the fare without outliers (#removed outliers={sum(mask_outliers)}) ')\n",
    "    plt.show()\n",
    "\n",
    "    # get the outliers of the remaining data\n",
    "    mask_outliers_it_2 = get_outliers_mask_IQR(df[~mask_outliers], 'fare')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Percentile\n",
    "\n",
    "The outliers are the data points that are outside the range $[P_{\\alpha/2}, P_{100-\\alpha/2}]$, where $P_{x}$ is the $x$-th percentile."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def get_outliers_mask_percentile(df, feature, alpha=0.05):\n",
    "    lower_bound = df[feature].quantile(alpha / 2)\n",
    "    upper_bound = df[feature].quantile(1 - alpha / 2)\n",
    "    mask_outliers = (df[feature] < lower_bound) | (df[feature] > upper_bound)\n",
    "    return mask_outliers\n",
    "\n",
    "\n",
    "mask_outliers = get_outliers_mask_percentile(df, 'fare', 0.02)\n",
    "outliers = df[mask_outliers]\n",
    "outliers.sort_values(by='fare', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Z-score\n",
    "\n",
    "The Z-score of the outliers is a measure of how many standard deviations away from the mean a data point is. The outliers are the data points that have a z-score greater than 3 or less than -3, i.e., $z > 3$ or $z < -3$, where $$z = \\frac{x - \\mu}{\\sigma},$$ $x$ is the data point, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "This is the same as considering the data points that are outside the range $[\\mu - 3 \\cdot \\sigma, \\mu + 3 \\cdot \\sigma]$.\n",
    "\n",
    "The 3 standard deviations rule is a common rule of thumb in statistics. It states that for a normal distribution, approximately 68% of the data points are within one standard deviation of the mean, approximately 95% of the data points are within two standard deviations of the mean, and approximately 99.7% of the data points are within three standard deviations of the mean.\n",
    "\n",
    "\n",
    "Let's see if there are outliers in the `age` feature using the z-score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def get_outliers_mask_z_score(df, feature, num_std=3):\n",
    "    lower_bound = df[feature].mean() - num_std * df[feature].std()\n",
    "    upper_bound = df[feature].mean() + num_std * df[feature].std()\n",
    "    mask_outliers = (df[feature] < lower_bound) | (df[feature] > upper_bound)\n",
    "    return mask_outliers\n",
    "\n",
    "\n",
    "mask_outliers = get_outliers_mask_z_score(df, 'fare')\n",
    "outliers = df[mask_outliers]\n",
    "outliers.sort_values(by='fare', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ax_0 = df['fare'].plot(kind='hist', bins=20)\n",
    "ax_0.set_title('Fare distribution ')\n",
    "\n",
    "ax_1 = df.loc[~mask_outliers, ['fare']].plot(kind='hist', bins=20)\n",
    "ax_1.set_title('Fare distribution without outliers')\n",
    "\n",
    "ax_2 = df.loc[mask_outliers, ['fare']].plot(kind='hist')\n",
    "ax_2.set_title('Fare distribution of the outliers')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As before, the process of removing the outliers can be done iteratively, i.e., removing the outliers and then removing the outliers of the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "mask_outliers_it_2 = get_outliers_mask_z_score(df[~mask_outliers], 'fare')\n",
    "while (mask_outliers_it_2.sum() > 0):\n",
    "    # update the outliers mask\n",
    "    mask_outliers = mask_outliers | mask_outliers_it_2\n",
    "    print('By now, we have removed {} outliers'.format(mask_outliers.sum()))\n",
    "    # plot the boxplot of the remaining data\n",
    "    df.loc[~mask_outliers, ['fare']].hist()\n",
    "    plt.show()\n",
    "    # get the outliers of the remaining data\n",
    "    mask_outliers_it_2 = get_outliers_mask_z_score(df[~mask_outliers], 'fare')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Maybe it was too much to remove all the outliers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Isolation forest\n",
    "\n",
    "Isoaltion forest is an unsupervised machine learning algorithm that is used to identify outliers. The outliers are the data points that are isolated from the rest of the data. The Isolation Forest process splits the data into smaller and smaller subsets until the data points are isolated. The less splits it takes to isolate a point, the more anomalous the point is. Comparing to the previous methods, IF allows to work with __high dimensional data__ and is __not sensitive to the distribution of the data__.\n",
    "\n",
    "![images/IF.png](images/IF.png)\n",
    "\n",
    "Isolation forest is implemented in the `sklearn` library. Let's see how it works.\n",
    "\n",
    "We will use the `age` and `fare` features to identify the outliers, helping to graphically visualize the results. However, isolation forest can be used with any number of features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# !pip install scikit-learn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# IF does not work with missing values, so we need to drop them\n",
    "df_for_if = df[['age', 'fare']].dropna().copy()\n",
    "\n",
    "# fit the model where the contamination is the percentage of outliers\n",
    "clf = IsolationForest(random_state=0, contamination=0.02)\n",
    "clf.fit(df_for_if)\n",
    "\n",
    "# predict the outliers\n",
    "y_pred = clf.predict(df_for_if)\n",
    "\n",
    "# add a column to the original dataframe identifying the outliers\n",
    "df_for_if['outlier'] = y_pred\n",
    "\n",
    "# show the outliers\n",
    "df_for_if[df_for_if.outlier == -1].sort_values(by='fare', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's see how the outliers are distributed in the `age` and `fare` features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df[['age', 'fare']].dropna().plot.scatter(x='age',\n",
    "                                          y='fare',\n",
    "                                          c=df_for_if['outlier'],\n",
    "                                          colormap='viridis')\n",
    "plt.title('Outliers in the age and fare features')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Autoencoder\n",
    "\n",
    "An autoencoder is a neural network that is used to learn the identity function. The autoencoder is trained to reconstruct the input data after passing through the hidden layers. The autoencoder is trained to minimize the reconstruction error, i.e., the difference between the input data and the reconstructed data. The outliers are the data points that are reconstructed with a high error.\n",
    "\n",
    "![images/AE.png](images/AE.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "list_of_features = ['age', 'fare', 'pclass', 'sibsp', 'parch', 'survived']\n",
    "df_for_AE = df[list_of_features].dropna().copy()\n",
    "\n",
    "# reproducibility with tensorflow\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "\n",
    "# define the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(len(list_of_features), activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#normalize the data to have zero mean and unit variance -- this is important for the autoencoder and the reconstruction error\n",
    "mu = df_for_AE.mean()\n",
    "std = df_for_AE.std()\n",
    "df_for_AE = (df_for_AE - mu) / std\n",
    "\n",
    "# fit the model where the input and the output are the same\n",
    "model.fit(df_for_AE, df_for_AE, epochs=30, verbose=1)\n",
    "\n",
    "# predict the reconstructed data\n",
    "y_pred = model.predict(df_for_AE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": "Now, compute the reconstruction error by computing the Euclidean distance between the original data and the reconstructed data."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reconstruction_error = df_for_AE - y_pred\n",
    "reconstruction_error['error'] = reconstruction_error.apply(lambda x: np.sqrt(np.sum(np.square(x))), axis=1)\n",
    "reconstruction_error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, identify the outliers by computing the quantile of the reconstruction error. Let us say that we want to remove the top 5% of the data points with the highest reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "mask_outliers = reconstruction_error['error'] > reconstruction_error['error'].quantile(0.95)\n",
    "reconstruction_error[mask_outliers].sort_values(by='error', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can add a column to the original data frame identifying the outliers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_for_AE['outlier'] = 1\n",
    "df_for_AE.loc[mask_outliers, 'outlier'] = -1\n",
    "df_for_AE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can plot the outliers in the `age` and `fare` features, as before."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_for_AE[['age', 'fare']].dropna().plot.scatter(x='age', y='fare', c=df_for_AE['outlier'], colormap='viridis')\n",
    "plt.title('Outliers in the age and fare features')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "Solve the exercises in the following notebooks:\n",
    "[Exercises/04_b_exercise_adult.ipynb](Exercises/04_b_exercise_adult.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# References\n",
    "\n",
    "- Campesato, O. (2018). Regular expressions: Pocket primer. Mercury Learning and Information.\n",
    "- https://www.kaggle.com/learn/pandas\n",
    "- Navlani, A.,  Fandango, A.,  Idris, I. (2021). Python Data Analysis: Perform data collection, data processing, wrangling, visualization, and model building using Python. Packt. 3rd Edition\n",
    "- Brandt. S. (2014). Data Analysis: Statistical and Computational Methods for Scientists and Engineers. Springer. 4th Edition\n",
    "- https://eugenelohh.medium.com/data-analysis-on-the-titanic-dataset-using-python-7593633135f2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_IDC_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
