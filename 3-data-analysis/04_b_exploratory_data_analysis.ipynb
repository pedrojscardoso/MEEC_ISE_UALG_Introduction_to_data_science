{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis with Pandas (part 2)\n",
    "\n",
    "Let us continue with the exploratory data analysis with Pandas. We will continue with the titanic dataset. So, load the titanic dataset and remember its structure."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_bokeh\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "pandas_bokeh.output_notebook()\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data visualization\n",
    "\n",
    "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\n",
    "\n",
    "Let us see how we can use data visualization to explore the titanic dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Line plot\n",
    "\n",
    "Line plots are useful to visualize the trend of a numerical column, usually, over time. We can use the `plot()` with the `kind='line'` argument from Pandas to visualize the trend of a numerical column.\n",
    "\n",
    "Since the Titanic dataset does not have a time column, we will use the Covid dataset to visualize the trend of the number of confirmed cases over time. The data is available at the \"COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University\" (https://github.com/CSSEGISandData/COVID-19)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# url with the data\n",
    "url_confirmed= 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "df_covid_all = pd.read_csv(url_confirmed, header=0)\n",
    "df_covid_all.head()\n",
    "#df[['age', 'sibsp', 'parch']].sort_values(by='age').set_index('age').plot(kind='line')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select some countries\n",
    "mask = df_covid_all['Country/Region'].isin(['Portugal', 'Spain', 'Italy', 'France', 'Germany', 'United Kingdom'])\n",
    "\n",
    "# group by country and sum the values, drop the lat and long columns, and transpose the dataframe\n",
    "df_covid = df_covid_all[mask] \\\n",
    "    .drop(['Lat', 'Long', 'Province/State'], axis=1) \\\n",
    "    .groupby('Country/Region').sum()\\\n",
    "    .T\n",
    "\n",
    "# set the index to datetime\n",
    "df_covid.set_index(pd.to_datetime(df_covid.index, format='%m/%d/%y'), inplace=True)\n",
    "\n",
    "df_covid.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_covid.plot(kind='line', figsize=(15, 5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_covid.plot(kind='area', figsize=(15, 5), alpha=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_covid.plot_bokeh(figsize=(800, 400), title='COVID-19')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_covid.plot_bokeh(figsize=(800, 400), kind='area', title='COVID-19')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scatter plot\n",
    "\n",
    "A scatter plot is useful to visualize the relationship between two numerical columns. We can use the `plot()` with the `kind='scatter'` argument from Pandas to visualize the relationship between two numerical columns.\n",
    "\n",
    "Let us return to the Titanic dataset and visualize the relationship between the age and the number of siblings and spouses aboard the Titanic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_titanic = pd.read_excel('./data/titanic/Titanic.xls')\n",
    "df_titanic.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_titanic.plot(kind='scatter', x='age', y='sibsp')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pie chart and donut chart\n",
    "\n",
    "Pie charts are useful to visualize the distribution of a categorical column. We can use the `plot()` with the `kind='pie'` argument from Pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = df_titanic.pivot_table(index='sex',\n",
    "               values='survived',\n",
    "               columns='pclass',\n",
    "               aggfunc='sum').plot(kind='pie', subplots=True, figsize=(10, 10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histograms\n",
    "We already have seen how to visualize the distribution of a numerical column using the `hist()` method from Pandas. We can also use the `hist()` method from Seaborn to visualize the distribution of a numerical column.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = df_titanic['age'].hist(alpha=0.75, bins=20, figsize=(10, 4), color='red')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Age distribution')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seaborn also provides the `histplot()` method to visualize the distribution of a numerical column, but with more options.\n",
    "(https://seaborn.pydata.org/generated/seaborn.histplot.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(data=df_titanic[['age']].dropna(), x='age', cumulative=True, bins=20, kde=True, element='step')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bar plot\n",
    "\n",
    "Bar plots are useful to visualize the distribution of a categorical column. We can use the `plot()` with the `kind='bar'` argument from Pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_titanic.pivot_table(index='sex',\n",
    "                                  values='survived',\n",
    "                                  columns='pclass',\n",
    "                                  aggfunc='sum').plot(kind='bar', figsize=(10, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_titanic.pivot_table(index='sex',\n",
    "                       values='survived',\n",
    "                       columns='pclass',\n",
    "                       aggfunc='sum').plot(kind='barh', figsize=(10, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Box plot\n",
    "\n",
    "Box plots are useful to visualize the distribution of a numerical column. We can use the `boxplot()` method from Pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_titanic[['age', 'fare']].boxplot(figsize=(10, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The seaborn library also provides the `boxplot()` method to visualize the distribution of a numerical column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_titanic, x='pclass', y='age')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kernel density estimation (KDE)\n",
    "Kernel density estimation is a non-parametric approach to estimating the distribution of data. Instead of assuming a particular distribution, we use a continuous representation of the data. For example, let's say we have a set of data measurements but we don't know their underlying distribution. We can use a Gaussian kernel to estimate the density around the data. If we apply this to a set of random data generated by a bimodal normal distribution, we can obtain an estimate of the distribution by summing the kernels.\n",
    "\n",
    "We can use the `kdeplot()` method from Seaborn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df_titanic, x='age', hue='pclass')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# not working on pandas 2.0.0?\n",
    "# df_titanic[['age']].dropna().plot.kde(figsize=(10, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Violin plot\n",
    "\n",
    "Violin plots are useful to visualize the distribution of a numerical column. Violin plots are similar to box plots, but they also show the probability density of the data at different values.\n",
    "\n",
    "We can use the `violinplot()` method from Seaborn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_titanic, x='pclass', y='age')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Heatmap\n",
    "\n",
    "Heatmaps are useful to visualize the correlation between numerical columns. We can use the `heatmap()` method from Seaborn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# numerical columns\n",
    "numerical_columns = df_titanic.select_dtypes(include=np.number).columns\n",
    "\n",
    "sns.heatmap(df_titanic[numerical_columns].corr(), annot=True, cmap='coolwarm')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pairplot\n",
    "\n",
    "Pairplots are useful to visualize the relationship between multiple numerical columns. We can use the `pairplot()` method from Seaborn. Several options are available to customize the pairplot, try some of them changing the `kind` and `diag_kind` arguments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(df_titanic,\n",
    "             kind='reg', # {‘scatter’, ‘kde’, ‘hist’, ‘reg’}\n",
    "             diag_kind='kde', # ‘hist’, ‘kde’,\n",
    "             hue='survived',\n",
    "             )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Jointplot\n",
    "Jointplots are useful to visualize the relationship between two numerical columns. We can use the `jointplot()` method from Seaborn which shows a scatter plot and the distribution of each variable. See the documentation for more options (https://seaborn.pydata.org/generated/seaborn.jointplot.html).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_titanic,\n",
    "              x='age', y='fare',\n",
    "              hue='survived',\n",
    "              kind='scatter' #{ “scatter” | “kde” | “hist” | “hex” | “reg” | “resid” }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hexbin plot\n",
    "\n",
    "Hexbin plots are useful to visualize the relationship between two numerical columns. We can use the `hexbin()` method from Pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# not working on pandas 2.0.0?\n",
    "# df_titanic[['age', 'fare']].dropna().plot.hexbin(x='age', y='fare', gridsize=20, figsize=(10, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bubble plot\n",
    "\n",
    "Bubble plots are useful to visualize the relationship between two numerical columns and a third numerical column. We can use the `scatter()` method from Pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_titanic.plot.scatter(x='age', y='fare', s='pclass', figsize=(10, 8))\n",
    "df_titanic.plot.scatter(x='age', y='fare',\n",
    "                        s=4**(4-df_titanic['pclass']),\n",
    "                        c=df_titanic['survived'].apply(lambda x: 'red' if x == 1 else 'blue'),\n",
    "                        figsize=(10, 8),\n",
    "                        alpha=0.5\n",
    "                        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning\n",
    "\n",
    "Real-world datasets are typically characterized by their messy and noisy nature, which often results in numerous faulty or missing values. Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset.\n",
    "\n",
    "Data cleaning is a fundamental step in the data analysis process. The data cleaning process involves many steps, such as:\n",
    "- Missing values treatment - remove or impute the missing values with values that are more representative of the data. E.g., the mean, the median, the mode, etc.\n",
    "\n",
    "- Outliers treatment - remove or transform the outliers.\n",
    "- Feature engineering - create new features from the existing features. E.g., create a new feature that is the sum or product of two other features.\n",
    "- Feature selection - remove features that are not useful for the analysis. E.g., remove features that have a low correlation with the target variable if the goal is to predict the target variable.\n",
    "- Feature scaling - scale the features to have the same range. E.g., scale the features to have values between 0 and 1. This is useful for some machine learning algorithms.\n",
    "- Feature transformation - transform the features to have a more normal distribution. E.g., transform the features to have a normal distribution. This is useful for some machine learning algorithms.\n",
    "- Feature discretization - discretize the features to have a more discrete distribution. E.g., transform real values to integer values. This is useful for some machine learning algorithms.\n",
    "- Feature extraction - extract features from the data. E.g., extract features from text data. This is useful for some machine learning algorithms.\n",
    "- Feature encoding - encode the features to have a more machine learning friendly format. E.g., encode the categorical features to have integer values.\n",
    "- Feature reduction - reduce the number of features. E.g., reduce the number of features using PCA. This is useful for some machine learning algorithms.\n",
    "- Feature aggregation - aggregate the features to have a more compact representation. E.g., aggregate the features using clustering. This is useful for some machine learning algorithms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing values treatment\n",
    "\n",
    "Values that are not present in the data are referred to as missing values. They may be absent due to a variety of reasons, including human error, privacy concerns, or the failure of the survey respondent to complete the value. Missing values pose a significant challenge in data science and are typically addressed during data preprocessing. E.g., missing values can negatively impact the performance of machine learning models.\n",
    "\n",
    "There are various methods to handle missing values, including **dropping** the records that contain missing values, manually **filling** in the missing values, using **measures of central tendency** such as mean, median, or mode to fill in the missing values, or employing **machine learning models** such as regression, decision trees, or KNNs to predict and fill in the missing values with the most probable value.\n",
    "\n",
    "Let's see how many missing values we have in each column. First, we can use the `isnull()` method from Pandas to get a boolean dataframe with True for the missing values and False for the non-missing values. Then, we can use the sum() method to get the number of missing values in each column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df_titanic.copy()\n",
    "\n",
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, we can use the `isna()` method from Pandas to get a boolean dataframe with True for the missing values and False for the non-missing values. Then, we can use the sum() method to get the number of missing values in each column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop missing values\n",
    "\n",
    "If we have a lot of data, we can drop the rows or columns that have missing values. We can use the `dropna()` method from Pandas to drop the rows or columns that have missing values. By default, the `dropna()` method drops the rows that have missing values. This will not work for the titanic dataset because every has at least one missing value, as we can see from the next cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding `axis=1` will drop the columns that have missing values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fill missing values\n",
    "\n",
    "If we have a small dataset, we can fill the missing values with values that are more representative of the data. We can use the `fillna()` method from Pandas to fill the missing values with values that are more representative of the data. E.g., the mean, the median, the mode, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the age and fare columns, were 263 values are unknown, we can fill the missing values with the mean age and median fare. If the data was normally distributed, we could have used the mean or median, but when the data is not normally distributed, we should use the median. Have a look at the histograms and skew values of the age and fare columns to see that age is \"more normally distributed\" than the fare. So impirically, we can use the mean for the age and the median for the fare."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "df['fare'].fillna(df['fare'].median(), inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embarked column is a categorical column, so we can fill the missing values with the most frequent value which is given by the mode.\n",
    "For the embarked column, we can fill the missing values with the most frequent value which is given by the mode."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the boat we can see that there are 823 missing values. This is because the boat column is only filled in for passengers who survived. We can fill the missing values with -1 as a value that represents that the passenger did not survive. We can also remove the boat column because if we think it is not useful for the analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['boat'].fillna(-1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Shapiro-Wilk test for normality (Note)\n",
    "\n",
    "To test the normality of the distribution we can use the Shapiro-Wilk test. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. If the p-value is less than an $\\alpha$ (e.g., a tipical value is $\\alpha = 0.05$), we can reject the null hypothesis and conclude that the data is not normally distributed. Otherwise, we **can not** reject the null hypothesis and conclude that the data is normally distributed. Usually, the significance level is set to 0.05 (5%), implying that it is acceptable to have a 5% probability of incorrectly rejecting the true null hypothesis. Further,  $p$ is basically the probability of finding our data if the null hypothesis is true.\n",
    "\n",
    "For our numerical variables, we can compute the p-values using the `shapiro()` function from the `scipy.stats` module. The `shapiro()` function returns the test statistic and the p-value. We can use the `select_dtypes()` method from Pandas to select only the numerical columns. Then, we can use a for loop to iterate over the numerical columns and print the p-value for each column. Further note the difference between original data and the data after filling the missing values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "def test_shapiro(df):\n",
    "    # for the numerical columns\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        p = shapiro(df[col].dropna())[1]\n",
    "        if p < 0.05:\n",
    "            print(f\"p-value for {col}: {p} (reject the null hypothesis, i.e., the sample does not look like a normal distribution)\")\n",
    "        else:\n",
    "            print(f\"p-value for {col}: {p}  (null hypothesis is not rejectable, i.e., sample looks like a normal distribution)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_shapiro(df_titanic)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_shapiro(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the Shapiro-Wilk test applied to some distributions in the next cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 5000\n",
    "df_NUBP =  pd.DataFrame({\n",
    "    'N':np.random.normal(0, 1, k),\n",
    "    'U':np.random.uniform(0, 1, k),\n",
    "    'B':np.random.binomial(100, 0.5, k),\n",
    "    'P':np.random.poisson(10, k),\n",
    "})\n",
    "\n",
    "# plot the histograms\n",
    "df_NUBP.hist(bins=20)\n",
    "\n",
    "# test the normality\n",
    "test_shapiro(df_NUBP)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outliers treatment\n",
    "\n",
    "Outliers are data points that are significantly different from the rest of the data. Outliers can be caused by measurement errors or by human errors. Outliers can have a significant impact on the analysis, so it is important to treat them. The outliers treatment can be done in two ways:\n",
    "- Remove the outliers.\n",
    "- Transform the outliers.\n",
    "\n",
    "To identify outliers we can use, e.g., the following methods:\n",
    "- Scatter plot - the scatter plot is a graphical representation of the relationship between two variables and it is used to identify outliers. The outliers are the data points that are far away from the rest of the data.\n",
    "- Interquartile range / Boxplot - the boxplot is a graphical representation of the distribution of the data. The boxplot shows the median, the first quartile, the third quartile, the minimum, the maximum, and the outliers. The outliers are the data points that are outside the range $[Q_1 - 1.5 \\cdot IQR, Q_3 + 1.5 \\cdot IQR]$, where $Q_1$ is the first quartile, $Q_3$ is the third quartile, and $IQR = Q_3 - Q_1$ is the interquartile range.\n",
    "- Percentile - the outliers are the data points that are outside the range $[P_{\\alpha/2}, P_{100-\\alpha/2}]$, where $P_{x}$ is the $x$-th percentile.\n",
    "- Z-score - the z-score is a measure of how many standard deviations away from the mean a data point is. The outliers are the data points that have a z-score greater than 3 or less than -3.\n",
    "- Isolation forest - the isolation forest is an unsupervised machine learning algorithm that is used to identify outliers. The outliers are the data points that are isolated from the rest of the data.\n",
    "- Auto encoder - the auto encoder is an unsupervised machine learning algorithm that is used to identify outliers. The outliers are the data points that are not reconstructed well by the auto encoder.\n",
    "\n",
    "Let us see some examples of outliers treatment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scatter plot\n",
    "\n",
    "If we plot the scatter plot of the `sibsp` (Number of siblings/spouses aboard) and the `parch` (Number of Parents/Children Aboard), we can see that there are a possible outlier with a `parch` equal to 9 and `sibsp` equal to 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# copy the original dataframe\n",
    "df = df_titanic.copy()\n",
    "\n",
    "_ = df.plot_bokeh.scatter(x='sibsp', y='parch')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see thar there is a single family that fits the description of the outlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[(df.parch == 9) & (df.sibsp == 1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By the ticket number, we can see the family's composition."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.ticket == 'CA. 2343']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  Interquartile range / Boxplot\n",
    "\n",
    "The boxplot is a graphical representation of the distribution of the data. The boxplot shows the median, the first quartile, the third quartile, the minimum, the maximum, and the outliers. The outliers are the data points that are outside the range $[Q_1 - 1.5 \\cdot IQR, Q_3 + 1.5 \\cdot IQR]$, where $Q_1$ is the first quartile, $Q_3$ is the third quartile, and $IQR = Q_3 - Q_1$ is the interquartile range.\n",
    "\n",
    "Using the boxplot, we can see that there are outliers in the `age`, `sibsp`, `parch`, and `fare`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = df.boxplot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, we can see those outliers in the `fare` feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_outliers_mask_IQR(df, feature):\n",
    "    q1 = df[feature].quantile(0.25)\n",
    "    q3 = df[feature].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    mask_outliers = (df[feature] < lower_bound) | (df[feature] > upper_bound)\n",
    "    return mask_outliers\n",
    "\n",
    "mask_outliers = get_outliers_mask_IQR(df, 'fare')\n",
    "outliers = df[mask_outliers]\n",
    "outliers.sort_values(by='fare', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing the outliers, we can see that the boxplot is more compact, although outliers are still visible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[~mask_outliers, ['fare']].boxplot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The process of removing the outliers can be done iteratively, i.e., removing the outliers and then removing the outliers of the remaining data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_outliers_it_2 = get_outliers_mask_IQR(df[~mask_outliers], 'fare')\n",
    "while (mask_outliers_it_2.sum() > 0):\n",
    "    # update the outliers mask\n",
    "    mask_outliers = mask_outliers | mask_outliers_it_2\n",
    "    # plot the boxplot of the remaining data\n",
    "    df.loc[~mask_outliers, ['fare']].boxplot()\n",
    "    plt.show()\n",
    "    # get the outliers of the remaining data\n",
    "    mask_outliers_it_2 = get_outliers_mask_IQR(df[~mask_outliers], 'fare')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Percentile\n",
    "\n",
    "The outliers are the data points that are outside the range $[P_{\\alpha/2}, P_{100-\\alpha/2}]$, where $P_{x}$ is the $x$-th percentile."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_outliers_mask_percentile(df, feature, alpha=0.05):\n",
    "    lower_bound = df[feature].quantile(alpha/2)\n",
    "    upper_bound = df[feature].quantile(1-alpha/2)\n",
    "    mask_outliers = (df[feature]<lower_bound) | (df[feature]>upper_bound)\n",
    "    return mask_outliers\n",
    "\n",
    "mask_outliers = get_outliers_mask_percentile(df, 'fare', 0.02)\n",
    "outliers = df[mask_outliers]\n",
    "outliers.sort_values(by='fare', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Z-score\n",
    "\n",
    "The Z-score of the outliers is a measure of how many standard deviations away from the mean a data point is. The outliers are the data points that have a z-score greater than 3 or less than -3, i.e., $z > 3$ or $z < -3$, where $z = \\frac{x - \\mu}{\\sigma}$, $x$ is the data point, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "Let's see if there are outliers in the `age` feature using the z-score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_outliers_mask_z_score(df, feature):\n",
    "    lower_bound = df[feature].mean() - 3 * df[feature].std()\n",
    "    upper_bound = df[feature].mean() + 3 * df[feature].std()\n",
    "    mask_outliers = (df[feature]<lower_bound) | (df[feature]>upper_bound)\n",
    "    return mask_outliers\n",
    "\n",
    "mask_outliers = get_outliers_mask_z_score(df, 'fare')\n",
    "outliers = df[mask_outliers]\n",
    "outliers.sort_values(by='fare', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[~mask_outliers, ['fare']].hist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, the process of removing the outliers can be done iteratively, i.e., removing the outliers and then removing the outliers of the remaining data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_outliers_it_2 = get_outliers_mask_z_score(df[~mask_outliers], 'fare')\n",
    "while (mask_outliers_it_2.sum() > 0):\n",
    "    # update the outliers mask\n",
    "    mask_outliers = mask_outliers | mask_outliers_it_2\n",
    "    print('By now, we have removed {} outliers'.format(mask_outliers.sum()))\n",
    "    # plot the boxplot of the remaining data\n",
    "    df.loc[~mask_outliers, ['fare']].hist()\n",
    "    plt.show()\n",
    "    # get the outliers of the remaining data\n",
    "    mask_outliers_it_2 = get_outliers_mask_z_score(df[~mask_outliers], 'fare')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maybe it was too much to remove all the outliers!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Isolation forest\n",
    "\n",
    "Isoaltion forest is an unsupervised machine learning algorithm that is used to identify outliers. The outliers are the data points that are isolated from the rest of the data. The Isolation Forest process splits the data into smaller and smaller subsets until the data points are isolated. The less splits it takes to isolate a point, the more anomalous the point is.\n",
    "\n",
    "![images/IF.png](images/IF.png)\n",
    "\n",
    "Isolation forest is implemented in the `sklearn` library. Let's see how it works. We will use the `age` and `fare` features to identify the outliers, helping to graphically visualize the results. However, isolation forest can be used with any number of features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df_for_if = df[['age', 'fare']].dropna().copy()\n",
    "\n",
    "# fit the model where the contamination is the percentage of outliers\n",
    "clf = IsolationForest(random_state=0, contamination=0.02)\n",
    "clf.fit(df_for_if)\n",
    "\n",
    "# predict the outliers\n",
    "y_pred = clf.predict(df_for_if)\n",
    "df_for_if['outlier'] = y_pred\n",
    "df_for_if[df_for_if.outlier == -1].sort_values(by='fare', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how the outliers are distributed in the `age` and `fare` features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['age', 'fare']].dropna().plot.scatter(x='age', y='fare', c=df_for_if['outlier'], colormap='viridis')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Autoencoder\n",
    "\n",
    "An autoencoder is a neural network that is used to learn the identity function. The autoencoder is trained to reconstruct the input data after passing through the hidden layers. The autoencoder is trained to minimize the reconstruction error, i.e., the difference between the input data and the reconstructed data. The outliers are the data points that are reconstructed with a high error.\n",
    "\n",
    "![images/AE.png](images/AE.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_features = ['age', 'fare', 'pclass', 'sibsp', 'parch', 'survived']\n",
    "df_for_AE = df[list_of_features].dropna().copy()\n",
    "\n",
    "\n",
    "# reproducibility with tensorflow\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "\n",
    "# define the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(len(list_of_features), activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#normalize the data to have zero mean and unit variance -- this is important for the autoencoder and the reconstruction error\n",
    "mu = df_for_AE.mean()\n",
    "std = df_for_AE.std()\n",
    "df_for_AE = (df_for_AE - mu) / std\n",
    "\n",
    "# fit the model where the input and the output are the same\n",
    "model.fit(df_for_AE, df_for_AE, epochs=20, verbose=1)\n",
    "\n",
    "# predict the reconstructed data\n",
    "y_pred = model.predict(df_for_AE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "|Now, compute the reconstruction error by computing the euclidean distance between the original data and the reconstructed data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reconstruction_error = df_for_AE - y_pred\n",
    "reconstruction_error['error'] = reconstruction_error.apply(lambda x: np.sqrt(np.sum(np.square(x))), axis=1)\n",
    "reconstruction_error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, identify the outliers by computing the quantile of the reconstruction error. Let us say that we want to remove the top 5% of the data points with the highest reconstruction error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_outliers = reconstruction_error['error'] > reconstruction_error['error'].quantile(0.95)\n",
    "reconstruction_error[mask_outliers].sort_values(by='error', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can add a column to the original data frame identifying the outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_for_AE['outlier'] = 1\n",
    "df_for_AE.loc[mask_outliers, 'outlier'] = -1\n",
    "df_for_AE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can plot the outliers in the `age` and `fare` features, as before."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_for_AE[['age', 'fare']].dropna().plot.scatter(x='age', y='fare', c=df_for_AE['outlier'], colormap='viridis')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises\n",
    "\n",
    "[05_exercise_adult_part_2.ipynb](05_exercise_adult_part_2.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    "- Campesato, O. (2018). Regular expressions: Pocket primer. Mercury Learning and Information.\n",
    "- https://www.kaggle.com/learn/pandas\n",
    "- Navlani, A.,  Fandango, A.,  Idris, I. (2021). Python Data Analysis: Perform data collection, data processing, wrangling, visualization, and model building using Python. Packt. 3rd Edition\n",
    "- Brandt. S. (2014). Data Analysis: Statistical and Computational Methods for Scientists and Engineers. Springer. 4th Edition\n",
    "- https://eugenelohh.medium.com/data-analysis-on-the-titanic-dataset-using-python-7593633135f2"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
